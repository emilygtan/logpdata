{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tv40msN4a_b0"
   },
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 4071,
     "status": "ok",
     "timestamp": 1533440739756,
     "user": {
      "displayName": "Emily T",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105098215025944251641"
     },
     "user_tz": 240
    },
    "id": "GwhO0j3UZV2W",
    "outputId": "82442c24-620c-4bbe-887e-72be125a2471"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.backend as K\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam\n",
    "import pubchempy as pcp\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 1605,
     "status": "ok",
     "timestamp": 1533440741400,
     "user": {
      "displayName": "Emily T",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105098215025944251641"
     },
     "user_tz": 240
    },
    "id": "vdjhneDwbDIw",
    "outputId": "b201214a-d012-4f81-b58b-09c0505cdfc3"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-dd4f9feff144>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-dd4f9feff144>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git clone https://github.com/odifmonster/Final_Project.git\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "git clone https://github.com/odifmonster/Final_Project.git\n",
    "os.chdir('Final_Project/Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1533440742572,
     "user": {
      "displayName": "Emily T",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105098215025944251641"
     },
     "user_tz": 240
    },
    "id": "fOX4KKqtbE_f",
    "outputId": "3bbd404c-e216-4a4d-c0ec-06b808d73425"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>LogP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C[C@H]([C@@H](C)Cl)Cl</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C(C=CBr)N</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCC(CO)Br</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[13CH3][13CH2][13CH2][13CH2][13CH2][13CH2]O</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCCOCCP</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        SMILES  LogP\n",
       "0                        C[C@H]([C@@H](C)Cl)Cl   2.3\n",
       "1                                    C(C=CBr)N   0.3\n",
       "2                                    CCC(CO)Br   1.3\n",
       "3  [13CH3][13CH2][13CH2][13CH2][13CH2][13CH2]O   2.0\n",
       "4                                      CCCOCCP   0.6"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv('raw_data.csv',header=None,names=['SMILES','LogP'])\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MTXjhC3kbLXt"
   },
   "source": [
    "#Convert SMILES to InChi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ix-pkTZ11ciu"
   },
   "outputs": [],
   "source": [
    "def checkin(): # Checking in with progress :)\n",
    "  if i % 1000 == 0:\n",
    "    print(i, end='...')\n",
    "  if (i > 0) and (i % 5000 == 0):\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 1027651,
     "status": "ok",
     "timestamp": 1533441771890,
     "user": {
      "displayName": "Emily T",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105098215025944251641"
     },
     "user_tz": 240
    },
    "id": "NpHmq7gMbKnB",
    "outputId": "cd708984-b0c2-4520-f1f6-a8b19c1c590a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...1000..."
     ]
    }
   ],
   "source": [
    "#mols dataframe\n",
    "data = raw_data\n",
    "for i in range(len(data)):\n",
    "  compound = pcp.get_compounds(raw_data.iloc[i,0], namespace='smiles')[0]\n",
    "  inchi = compound.inchi\n",
    "  data.iloc[i,0] = inchi\n",
    "  checkin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1533441772244,
     "user": {
      "displayName": "Emily T",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105098215025944251641"
     },
     "user_tz": 240
    },
    "id": "fhnzYEj4JrMt",
    "outputId": "9de5e538-0197-4b40-e2cd-26104723819f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>LogP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>InChI=1S/C4H8Cl2/c1-3(5)4(2)6/h3-4H,1-2H3/t3-,...</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>InChI=1S/C3H6BrN/c4-2-1-3-5/h1-2H,3,5H2</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>InChI=1S/C4H9BrO/c1-2-4(5)3-6/h4,6H,2-3H2,1H3</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>InChI=1S/C6H14O/c1-2-3-4-5-6-7/h7H,2-6H2,1H3/i...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>InChI=1S/C5H13OP/c1-2-3-6-4-5-7/h2-5,7H2,1H3</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              SMILES  LogP\n",
       "0  InChI=1S/C4H8Cl2/c1-3(5)4(2)6/h3-4H,1-2H3/t3-,...   2.3\n",
       "1            InChI=1S/C3H6BrN/c4-2-1-3-5/h1-2H,3,5H2   0.3\n",
       "2      InChI=1S/C4H9BrO/c1-2-4(5)3-6/h4,6H,2-3H2,1H3   1.3\n",
       "3  InChI=1S/C6H14O/c1-2-3-4-5-6-7/h7H,2-6H2,1H3/i...   2.0\n",
       "4       InChI=1S/C5H13OP/c1-2-3-6-4-5-7/h2-5,7H2,1H3   0.6"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S_RICfVabW09"
   },
   "source": [
    "#Bigram Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "bOTdEdQ1b1ny"
   },
   "outputs": [],
   "source": [
    "bigram_dict = {}\n",
    "\n",
    "# Assigning bigrams to dictionary\n",
    "for i in range(len(data)): # Iterating over compounds\n",
    "  compound = data['SMILES'][i]\n",
    "  bigrams = [zip(list(compound)[:-1], list(compound)[1:])][0]\n",
    "  \n",
    "  for b in bigrams: # Iterating over each bigram in compound\n",
    "    bigram_dict.update({b:0})\n",
    "    \n",
    "features = np.ndarray(shape=(len(data), len(bigram_dict)))\n",
    "\n",
    "# Custom dictionary per compound (counting occurences of bigrams)\n",
    "for i in range(len(data)):\n",
    "  # Assembling comp_dict\n",
    "  comp_dict = bigram_dict.copy()\n",
    "  compound = data.iloc[i,0]\n",
    "  bigrams = [zip(list(compound)[:-1], list(compound)[1:])][0]\n",
    "  \n",
    "  for b in bigrams:\n",
    "    comp_dict[b] += 1\n",
    "    \n",
    "  # Add to features vector\n",
    "  features[i] = list(comp_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QDly3p5xb7py"
   },
   "source": [
    "#Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "UetSSRTLcC-M"
   },
   "outputs": [],
   "source": [
    "labels = data['LogP'].values\n",
    "\n",
    "# split to train & test data\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n",
    "\n",
    "#split to train & validation data\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IfwNigRmZLoX"
   },
   "source": [
    "#ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AVHVv_YEZM-v"
   },
   "outputs": [],
   "source": [
    "# RMSE for metrics\n",
    "def rmse(y_true, y_pred):\n",
    "  return K.sqrt(K.mean((y_true - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 731,
     "status": "ok",
     "timestamp": 1533441778966,
     "user": {
      "displayName": "Emily T",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105098215025944251641"
     },
     "user_tz": 240
    },
    "id": "gcSlCgxfZOUq",
    "outputId": "ce9a35b9-063b-457b-f568-693d31ba9bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation: softplus\n",
      "Dropout:    0.1\n",
      "Loss:       logcosh\n",
      "Optimizer:  Adadelta w/ LR 0.008\n",
      "Epochs:     1600\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Dense_1 (Dense)              (None, 512)               302592    \n",
      "_________________________________________________________________\n",
      "Dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "Dense_2 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "Dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "Dense_3 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "Dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "Dense_4 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "Dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "Dense_5 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "Dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 477,185\n",
      "Trainable params: 477,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# -- Settings -- #\n",
    "#        act drop loss l_r opt epochs\n",
    "ann_d = [2,  0,   3,   2,  3,  1600]\n",
    "\n",
    "#         0         1         2          3       4\n",
    "activ = ['softmax','sigmoid','softplus','relu', 'linear'][ann_d[0]]\n",
    "#don't use: softmax,\n",
    "\n",
    "#          0   1   2    3\n",
    "dropout = [0.1,0.2,0.3,.15][ann_d[1]]\n",
    "\n",
    "loss = ['mean_squared_error',             # 0\n",
    "        'mean_absolute_error',            # 1\n",
    "        'mean_absolute_percentage_error', # 2\n",
    "        'logcosh',                        # 3\n",
    "        'poisson',                        # 4\n",
    "        'cosine_proximity'][ann_d[2]]     # 5\n",
    "\n",
    "#      0    1    2     3     4     5      6\n",
    "l_r = [0.02,0.01,0.008,0.005,0.001,0.0005,0.0001][ann_d[3]]\n",
    "\n",
    "optimizer = [tf.keras.optimizers.SGD(l_r),            # 0\n",
    "             tf.keras.optimizers.RMSprop(l_r),        # 1\n",
    "             tf.keras.optimizers.Adagrad(),           # 2\n",
    "             tf.keras.optimizers.Adadelta(),          # 3\n",
    "             tf.keras.optimizers.Adam(l_r)][ann_d[4]] # 4\n",
    "\n",
    "#         0  1  2\n",
    "#epochs = [60,80,100][ann_d[5]]\n",
    "epochs = ann_d[5]\n",
    "\n",
    "# -- Layers -- #\n",
    "ann_model = tf.keras.Sequential()\n",
    "\n",
    "ann_model.add(tf.keras.layers.Dense(512, activation=activ, name = 'Dense_1', input_shape=features[0].shape))\n",
    "ann_model.add(tf.keras.layers.Dropout(dropout, name = 'Dropout_1'))\n",
    "\n",
    "ann_model.add(tf.keras.layers.Dense(256, activation=activ, name = 'Dense_2'))\n",
    "ann_model.add(tf.keras.layers.Dropout(dropout, name = 'Dropout_2'))\n",
    "\n",
    "ann_model.add(tf.keras.layers.Dense(128, activation=activ, name = 'Dense_3'))\n",
    "ann_model.add(tf.keras.layers.Dropout(dropout, name = 'Dropout_3'))\n",
    "\n",
    "ann_model.add(tf.keras.layers.Dense(64, activation=activ, name = 'Dense_4'))\n",
    "ann_model.add(tf.keras.layers.Dropout(dropout, name = 'Dropout_4'))\n",
    "\n",
    "ann_model.add(tf.keras.layers.Dense(32, activation=activ, name = 'Dense_5'))\n",
    "ann_model.add(tf.keras.layers.Dropout(dropout, name = 'Dropout_5'))\n",
    "\n",
    "ann_model.add(tf.keras.layers.Dense(1, activation='linear', name = 'Output'))\n",
    "\n",
    "# -- Compile -- #\n",
    "ann_model.compile(loss=loss, optimizer=optimizer, metrics=[rmse])\n",
    "\n",
    "print('Activation:', activ)\n",
    "print('Dropout:   ', dropout)\n",
    "print('Loss:      ', loss)\n",
    "print('Optimizer: ', optimizer.__class__.__name__, 'w/ LR', l_r)\n",
    "print('Epochs:    ', epochs)\n",
    "ann_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 15388
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "en-JcttiZr1k",
    "outputId": "0816d76a-1c51-40f6-d141-4de9e26d3c65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6573 samples, validate on 1644 samples\n",
      "Epoch 1/1600\n",
      "6573/6573 [==============================] - 3s 531us/step - loss: 0.5676 - rmse: 1.2898 - val_loss: 0.4217 - val_rmse: 1.0620\n",
      "Epoch 2/1600\n",
      "6573/6573 [==============================] - 3s 476us/step - loss: 0.3093 - rmse: 0.8882 - val_loss: 0.4019 - val_rmse: 1.0292\n",
      "Epoch 3/1600\n",
      "6573/6573 [==============================] - 3s 448us/step - loss: 0.2297 - rmse: 0.7484 - val_loss: 0.3652 - val_rmse: 0.9793\n",
      "Epoch 4/1600\n",
      "6573/6573 [==============================] - 3s 458us/step - loss: 0.1983 - rmse: 0.6866 - val_loss: 0.2109 - val_rmse: 0.7149\n",
      "Epoch 5/1600\n",
      "4352/6573 [==================>...........] - ETA: 1s - loss: 0.1799 - rmse: 0.65016573/6573 [==============================] - 3s 481us/step - loss: 0.1749 - rmse: 0.6394 - val_loss: 0.1342 - val_rmse: 0.5578\n",
      "Epoch 6/1600\n",
      "6573/6573 [==============================] - 3s 478us/step - loss: 0.1608 - rmse: 0.6097 - val_loss: 0.2455 - val_rmse: 0.7720\n",
      "Epoch 7/1600\n",
      "6573/6573 [==============================] - 3s 469us/step - loss: 0.1501 - rmse: 0.5860 - val_loss: 0.1268 - val_rmse: 0.5388\n",
      "Epoch 8/1600\n",
      "6573/6573 [==============================] - 3s 471us/step - loss: 0.1430 - rmse: 0.5712 - val_loss: 0.1225 - val_rmse: 0.5297\n",
      "Epoch 9/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.1360 - rmse: 0.5542 - val_loss: 0.1218 - val_rmse: 0.5270\n",
      "Epoch 10/1600\n",
      " 800/6573 [==>...........................] - ETA: 2s - loss: 0.1265 - rmse: 0.53066573/6573 [==============================] - 3s 443us/step - loss: 0.1291 - rmse: 0.5387 - val_loss: 0.1383 - val_rmse: 0.5650\n",
      "Epoch 11/1600\n",
      "6573/6573 [==============================] - 3s 466us/step - loss: 0.1221 - rmse: 0.5237 - val_loss: 0.1283 - val_rmse: 0.5429\n",
      "Epoch 12/1600\n",
      "6573/6573 [==============================] - 3s 476us/step - loss: 0.1202 - rmse: 0.5190 - val_loss: 0.1072 - val_rmse: 0.4928\n",
      "Epoch 13/1600\n",
      "6573/6573 [==============================] - 3s 467us/step - loss: 0.1132 - rmse: 0.5015 - val_loss: 0.1792 - val_rmse: 0.6442\n",
      "Epoch 14/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.1128 - rmse: 0.4998 - val_loss: 0.0962 - val_rmse: 0.4652\n",
      "Epoch 15/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.1021 - rmse: 0.47566573/6573 [==============================] - 3s 471us/step - loss: 0.1086 - rmse: 0.4900 - val_loss: 0.1428 - val_rmse: 0.5731\n",
      "Epoch 16/1600\n",
      "6573/6573 [==============================] - 3s 456us/step - loss: 0.1053 - rmse: 0.4818 - val_loss: 0.1183 - val_rmse: 0.5199\n",
      "Epoch 17/1600\n",
      "6573/6573 [==============================] - 3s 439us/step - loss: 0.1032 - rmse: 0.4768 - val_loss: 0.1204 - val_rmse: 0.5229\n",
      "Epoch 18/1600\n",
      "6573/6573 [==============================] - 3s 462us/step - loss: 0.0996 - rmse: 0.4673 - val_loss: 0.0892 - val_rmse: 0.4482\n",
      "Epoch 19/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0992 - rmse: 0.4664 - val_loss: 0.1025 - val_rmse: 0.4812\n",
      "Epoch 20/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0968 - rmse: 0.46046573/6573 [==============================] - 3s 480us/step - loss: 0.0968 - rmse: 0.4613 - val_loss: 0.1188 - val_rmse: 0.5167\n",
      "Epoch 21/1600\n",
      "6573/6573 [==============================] - 3s 467us/step - loss: 0.0947 - rmse: 0.4552 - val_loss: 0.1059 - val_rmse: 0.4867\n",
      "Epoch 22/1600\n",
      "6573/6573 [==============================] - 3s 478us/step - loss: 0.0938 - rmse: 0.4517 - val_loss: 0.0870 - val_rmse: 0.4407\n",
      "Epoch 23/1600\n",
      "6573/6573 [==============================] - 3s 460us/step - loss: 0.0928 - rmse: 0.4500 - val_loss: 0.0855 - val_rmse: 0.4374\n",
      "Epoch 24/1600\n",
      "6573/6573 [==============================] - 3s 452us/step - loss: 0.0907 - rmse: 0.4442 - val_loss: 0.0831 - val_rmse: 0.4296\n",
      "Epoch 25/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0783 - rmse: 0.40956573/6573 [==============================] - 3s 482us/step - loss: 0.0890 - rmse: 0.4395 - val_loss: 0.0978 - val_rmse: 0.4694\n",
      "Epoch 26/1600\n",
      "6573/6573 [==============================] - 3s 477us/step - loss: 0.0878 - rmse: 0.4383 - val_loss: 0.0775 - val_rmse: 0.4160\n",
      "Epoch 27/1600\n",
      "6573/6573 [==============================] - 3s 481us/step - loss: 0.0870 - rmse: 0.4352 - val_loss: 0.0928 - val_rmse: 0.4564\n",
      "Epoch 28/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0857 - rmse: 0.4298 - val_loss: 0.0817 - val_rmse: 0.4268\n",
      "Epoch 29/1600\n",
      "6573/6573 [==============================] - 3s 476us/step - loss: 0.0830 - rmse: 0.4238 - val_loss: 0.0803 - val_rmse: 0.4215\n",
      "Epoch 30/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0776 - rmse: 0.41096573/6573 [==============================] - 3s 437us/step - loss: 0.0833 - rmse: 0.4242 - val_loss: 0.0834 - val_rmse: 0.4289\n",
      "Epoch 31/1600\n",
      "6573/6573 [==============================] - 3s 447us/step - loss: 0.0831 - rmse: 0.4238 - val_loss: 0.0768 - val_rmse: 0.4122\n",
      "Epoch 32/1600\n",
      "6573/6573 [==============================] - 3s 463us/step - loss: 0.0824 - rmse: 0.4222 - val_loss: 0.0772 - val_rmse: 0.4138\n",
      "Epoch 33/1600\n",
      "6573/6573 [==============================] - 3s 467us/step - loss: 0.0795 - rmse: 0.4141 - val_loss: 0.0777 - val_rmse: 0.4148\n",
      "Epoch 34/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0806 - rmse: 0.4169 - val_loss: 0.0892 - val_rmse: 0.4471\n",
      "Epoch 35/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0779 - rmse: 0.40826573/6573 [==============================] - 3s 467us/step - loss: 0.0774 - rmse: 0.4080 - val_loss: 0.0828 - val_rmse: 0.4280\n",
      "Epoch 36/1600\n",
      "6573/6573 [==============================] - 3s 476us/step - loss: 0.0783 - rmse: 0.4112 - val_loss: 0.0776 - val_rmse: 0.4138\n",
      "Epoch 37/1600\n",
      "6573/6573 [==============================] - 3s 427us/step - loss: 0.0768 - rmse: 0.4056 - val_loss: 0.0760 - val_rmse: 0.4096\n",
      "Epoch 38/1600\n",
      "6573/6573 [==============================] - 3s 450us/step - loss: 0.0762 - rmse: 0.4053 - val_loss: 0.0821 - val_rmse: 0.4276\n",
      "Epoch 39/1600\n",
      "6573/6573 [==============================] - 3s 462us/step - loss: 0.0776 - rmse: 0.4091 - val_loss: 0.0727 - val_rmse: 0.4007\n",
      "Epoch 40/1600\n",
      " 384/6573 [>.............................] - ETA: 3s - loss: 0.0764 - rmse: 0.40656573/6573 [==============================] - 3s 475us/step - loss: 0.0752 - rmse: 0.4012 - val_loss: 0.0722 - val_rmse: 0.3986\n",
      "Epoch 41/1600\n",
      "6573/6573 [==============================] - 3s 466us/step - loss: 0.0753 - rmse: 0.4020 - val_loss: 0.0744 - val_rmse: 0.4065\n",
      "Epoch 42/1600\n",
      "6573/6573 [==============================] - 3s 471us/step - loss: 0.0735 - rmse: 0.3968 - val_loss: 0.0712 - val_rmse: 0.3979\n",
      "Epoch 43/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0733 - rmse: 0.3969 - val_loss: 0.0782 - val_rmse: 0.4146\n",
      "Epoch 44/1600\n",
      "6573/6573 [==============================] - 3s 445us/step - loss: 0.0732 - rmse: 0.3956 - val_loss: 0.0712 - val_rmse: 0.3961\n",
      "Epoch 45/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0758 - rmse: 0.40416573/6573 [==============================] - 3s 459us/step - loss: 0.0738 - rmse: 0.3983 - val_loss: 0.0762 - val_rmse: 0.4109\n",
      "Epoch 46/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0718 - rmse: 0.3908 - val_loss: 0.0834 - val_rmse: 0.4285\n",
      "Epoch 47/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0715 - rmse: 0.3906 - val_loss: 0.0674 - val_rmse: 0.3850\n",
      "Epoch 48/1600\n",
      "6573/6573 [==============================] - 3s 477us/step - loss: 0.0703 - rmse: 0.3879 - val_loss: 0.0775 - val_rmse: 0.4139\n",
      "Epoch 49/1600\n",
      "6573/6573 [==============================] - 3s 478us/step - loss: 0.0717 - rmse: 0.3914 - val_loss: 0.0729 - val_rmse: 0.4016\n",
      "Epoch 50/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0766 - rmse: 0.40436573/6573 [==============================] - 3s 464us/step - loss: 0.0711 - rmse: 0.3895 - val_loss: 0.0770 - val_rmse: 0.4128\n",
      "Epoch 51/1600\n",
      "6573/6573 [==============================] - 3s 446us/step - loss: 0.0699 - rmse: 0.3866 - val_loss: 0.0672 - val_rmse: 0.3841\n",
      "Epoch 52/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.0682 - rmse: 0.3814 - val_loss: 0.0758 - val_rmse: 0.4083\n",
      "Epoch 53/1600\n",
      "6573/6573 [==============================] - 3s 465us/step - loss: 0.0686 - rmse: 0.3823 - val_loss: 0.0714 - val_rmse: 0.3962\n",
      "Epoch 54/1600\n",
      "6573/6573 [==============================] - 3s 477us/step - loss: 0.0667 - rmse: 0.3764 - val_loss: 0.0738 - val_rmse: 0.4054\n",
      "Epoch 55/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0653 - rmse: 0.37196573/6573 [==============================] - 3s 463us/step - loss: 0.0663 - rmse: 0.3749 - val_loss: 0.0736 - val_rmse: 0.4028\n",
      "Epoch 56/1600\n",
      "6573/6573 [==============================] - 3s 469us/step - loss: 0.0670 - rmse: 0.3782 - val_loss: 0.0701 - val_rmse: 0.3926\n",
      "Epoch 57/1600\n",
      "6573/6573 [==============================] - 3s 454us/step - loss: 0.0667 - rmse: 0.3769 - val_loss: 0.0777 - val_rmse: 0.4139\n",
      "Epoch 58/1600\n",
      "6573/6573 [==============================] - 3s 440us/step - loss: 0.0654 - rmse: 0.3734 - val_loss: 0.0683 - val_rmse: 0.3877\n",
      "Epoch 59/1600\n",
      "6573/6573 [==============================] - 3s 483us/step - loss: 0.0655 - rmse: 0.3726 - val_loss: 0.0666 - val_rmse: 0.3832\n",
      "Epoch 60/1600\n",
      " 544/6573 [=>............................] - ETA: 2s - loss: 0.0704 - rmse: 0.38736573/6573 [==============================] - 3s 486us/step - loss: 0.0643 - rmse: 0.3693 - val_loss: 0.0709 - val_rmse: 0.3936\n",
      "Epoch 61/1600\n",
      "6573/6573 [==============================] - 3s 488us/step - loss: 0.0653 - rmse: 0.3725 - val_loss: 0.0732 - val_rmse: 0.4012\n",
      "Epoch 62/1600\n",
      "6573/6573 [==============================] - 3s 487us/step - loss: 0.0644 - rmse: 0.3694 - val_loss: 0.0972 - val_rmse: 0.4657\n",
      "Epoch 63/1600\n",
      "6573/6573 [==============================] - 3s 483us/step - loss: 0.0639 - rmse: 0.3679 - val_loss: 0.0640 - val_rmse: 0.3743\n",
      "Epoch 64/1600\n",
      "6573/6573 [==============================] - 3s 451us/step - loss: 0.0653 - rmse: 0.3727 - val_loss: 0.0651 - val_rmse: 0.3781\n",
      "Epoch 65/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0647 - rmse: 0.37106573/6573 [==============================] - 3s 477us/step - loss: 0.0635 - rmse: 0.3677 - val_loss: 0.0679 - val_rmse: 0.3857\n",
      "Epoch 66/1600\n",
      "6573/6573 [==============================] - 3s 485us/step - loss: 0.0624 - rmse: 0.3642 - val_loss: 0.0711 - val_rmse: 0.3946\n",
      "Epoch 67/1600\n",
      "6573/6573 [==============================] - 3s 479us/step - loss: 0.0637 - rmse: 0.3675 - val_loss: 0.0626 - val_rmse: 0.3714\n",
      "Epoch 68/1600\n",
      "6573/6573 [==============================] - 3s 477us/step - loss: 0.0618 - rmse: 0.3622 - val_loss: 0.0736 - val_rmse: 0.4020\n",
      "Epoch 69/1600\n",
      "6573/6573 [==============================] - 3s 476us/step - loss: 0.0604 - rmse: 0.3566 - val_loss: 0.0627 - val_rmse: 0.3711\n",
      "Epoch 70/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0636 - rmse: 0.36826573/6573 [==============================] - 3s 463us/step - loss: 0.0625 - rmse: 0.3642 - val_loss: 0.0713 - val_rmse: 0.3957\n",
      "Epoch 71/1600\n",
      "6573/6573 [==============================] - 3s 439us/step - loss: 0.0615 - rmse: 0.3609 - val_loss: 0.0709 - val_rmse: 0.3939\n",
      "Epoch 72/1600\n",
      "6573/6573 [==============================] - 3s 460us/step - loss: 0.0610 - rmse: 0.3596 - val_loss: 0.0629 - val_rmse: 0.3716\n",
      "Epoch 73/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0596 - rmse: 0.3542 - val_loss: 0.0652 - val_rmse: 0.3771\n",
      "Epoch 74/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0607 - rmse: 0.3581 - val_loss: 0.0646 - val_rmse: 0.3761\n",
      "Epoch 75/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0557 - rmse: 0.34306573/6573 [==============================] - 3s 465us/step - loss: 0.0596 - rmse: 0.3547 - val_loss: 0.0623 - val_rmse: 0.3697\n",
      "Epoch 76/1600\n",
      "6573/6573 [==============================] - 3s 471us/step - loss: 0.0606 - rmse: 0.3583 - val_loss: 0.0657 - val_rmse: 0.3813\n",
      "Epoch 77/1600\n",
      "6573/6573 [==============================] - 3s 457us/step - loss: 0.0584 - rmse: 0.3508 - val_loss: 0.0746 - val_rmse: 0.4058\n",
      "Epoch 78/1600\n",
      "6573/6573 [==============================] - 3s 438us/step - loss: 0.0584 - rmse: 0.3508 - val_loss: 0.0657 - val_rmse: 0.3790\n",
      "Epoch 79/1600\n",
      "6573/6573 [==============================] - 3s 469us/step - loss: 0.0587 - rmse: 0.3516 - val_loss: 0.0610 - val_rmse: 0.3657\n",
      "Epoch 80/1600\n",
      " 544/6573 [=>............................] - ETA: 2s - loss: 0.0553 - rmse: 0.34066573/6573 [==============================] - 3s 478us/step - loss: 0.0583 - rmse: 0.3507 - val_loss: 0.0812 - val_rmse: 0.4209\n",
      "Epoch 81/1600\n",
      "6573/6573 [==============================] - 3s 483us/step - loss: 0.0588 - rmse: 0.3520 - val_loss: 0.0702 - val_rmse: 0.3924\n",
      "Epoch 82/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0573 - rmse: 0.3471 - val_loss: 0.0712 - val_rmse: 0.3968\n",
      "Epoch 83/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0578 - rmse: 0.3487 - val_loss: 0.0624 - val_rmse: 0.3694\n",
      "Epoch 84/1600\n",
      "6573/6573 [==============================] - 3s 449us/step - loss: 0.0565 - rmse: 0.3442 - val_loss: 0.0650 - val_rmse: 0.3766\n",
      "Epoch 85/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0596 - rmse: 0.35646573/6573 [==============================] - 3s 444us/step - loss: 0.0582 - rmse: 0.3503 - val_loss: 0.0615 - val_rmse: 0.3679\n",
      "Epoch 86/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0572 - rmse: 0.3476 - val_loss: 0.0624 - val_rmse: 0.3703\n",
      "Epoch 87/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0577 - rmse: 0.3489 - val_loss: 0.0597 - val_rmse: 0.3612\n",
      "Epoch 88/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0570 - rmse: 0.3468 - val_loss: 0.0623 - val_rmse: 0.3694\n",
      "Epoch 89/1600\n",
      "6573/6573 [==============================] - 3s 464us/step - loss: 0.0556 - rmse: 0.3427 - val_loss: 0.0656 - val_rmse: 0.3779\n",
      "Epoch 90/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0606 - rmse: 0.36006573/6573 [==============================] - 3s 469us/step - loss: 0.0569 - rmse: 0.3467 - val_loss: 0.0616 - val_rmse: 0.3673\n",
      "Epoch 91/1600\n",
      "6573/6573 [==============================] - 3s 437us/step - loss: 0.0564 - rmse: 0.3447 - val_loss: 0.0589 - val_rmse: 0.3580\n",
      "Epoch 92/1600\n",
      "6573/6573 [==============================] - 3s 454us/step - loss: 0.0558 - rmse: 0.3428 - val_loss: 0.0632 - val_rmse: 0.3718\n",
      "Epoch 93/1600\n",
      "6573/6573 [==============================] - 3s 479us/step - loss: 0.0559 - rmse: 0.3429 - val_loss: 0.0610 - val_rmse: 0.3649\n",
      "Epoch 94/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0547 - rmse: 0.3387 - val_loss: 0.0644 - val_rmse: 0.3753\n",
      "Epoch 95/1600\n",
      " 384/6573 [>.............................] - ETA: 2s - loss: 0.0551 - rmse: 0.34096573/6573 [==============================] - 3s 468us/step - loss: 0.0547 - rmse: 0.3393 - val_loss: 0.0665 - val_rmse: 0.3803\n",
      "Epoch 96/1600\n",
      "6573/6573 [==============================] - 3s 464us/step - loss: 0.0549 - rmse: 0.3395 - val_loss: 0.0630 - val_rmse: 0.3709\n",
      "Epoch 97/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0530 - rmse: 0.3324 - val_loss: 0.0609 - val_rmse: 0.3640\n",
      "Epoch 98/1600\n",
      "6573/6573 [==============================] - 3s 441us/step - loss: 0.0528 - rmse: 0.3333 - val_loss: 0.0634 - val_rmse: 0.3713\n",
      "Epoch 99/1600\n",
      "6573/6573 [==============================] - 3s 450us/step - loss: 0.0549 - rmse: 0.3394 - val_loss: 0.0640 - val_rmse: 0.3724\n",
      "Epoch 100/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0608 - rmse: 0.36416573/6573 [==============================] - 3s 472us/step - loss: 0.0534 - rmse: 0.3352 - val_loss: 0.0573 - val_rmse: 0.3533\n",
      "Epoch 101/1600\n",
      "6573/6573 [==============================] - 3s 479us/step - loss: 0.0546 - rmse: 0.3389 - val_loss: 0.0576 - val_rmse: 0.3539\n",
      "Epoch 102/1600\n",
      "6573/6573 [==============================] - 3s 465us/step - loss: 0.0526 - rmse: 0.3323 - val_loss: 0.0592 - val_rmse: 0.3580\n",
      "Epoch 103/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0520 - rmse: 0.3297 - val_loss: 0.0674 - val_rmse: 0.3844\n",
      "Epoch 104/1600\n",
      "6573/6573 [==============================] - 3s 462us/step - loss: 0.0526 - rmse: 0.3309 - val_loss: 0.0594 - val_rmse: 0.3592\n",
      "Epoch 105/1600\n",
      "  32/6573 [..............................] - ETA: 3s - loss: 0.0433 - rmse: 0.29946573/6573 [==============================] - 3s 438us/step - loss: 0.0525 - rmse: 0.3316 - val_loss: 0.0566 - val_rmse: 0.3510\n",
      "Epoch 106/1600\n",
      "6573/6573 [==============================] - 3s 464us/step - loss: 0.0534 - rmse: 0.3354 - val_loss: 0.0607 - val_rmse: 0.3625\n",
      "Epoch 107/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0522 - rmse: 0.3303 - val_loss: 0.0573 - val_rmse: 0.3531\n",
      "Epoch 108/1600\n",
      "6573/6573 [==============================] - 3s 471us/step - loss: 0.0522 - rmse: 0.3313 - val_loss: 0.0597 - val_rmse: 0.3607\n",
      "Epoch 109/1600\n",
      "6573/6573 [==============================] - 3s 469us/step - loss: 0.0515 - rmse: 0.3278 - val_loss: 0.0554 - val_rmse: 0.3470\n",
      "Epoch 110/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0518 - rmse: 0.33156573/6573 [==============================] - 3s 468us/step - loss: 0.0510 - rmse: 0.3272 - val_loss: 0.0599 - val_rmse: 0.3604\n",
      "Epoch 111/1600\n",
      "6573/6573 [==============================] - 3s 457us/step - loss: 0.0511 - rmse: 0.3270 - val_loss: 0.0574 - val_rmse: 0.3529\n",
      "Epoch 112/1600\n",
      "6573/6573 [==============================] - 3s 433us/step - loss: 0.0502 - rmse: 0.3238 - val_loss: 0.0578 - val_rmse: 0.3548\n",
      "Epoch 113/1600\n",
      "6573/6573 [==============================] - 3s 463us/step - loss: 0.0500 - rmse: 0.3232 - val_loss: 0.0563 - val_rmse: 0.3495\n",
      "Epoch 114/1600\n",
      "6573/6573 [==============================] - 3s 467us/step - loss: 0.0511 - rmse: 0.3275 - val_loss: 0.0699 - val_rmse: 0.3902\n",
      "Epoch 115/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0387 - rmse: 0.27846573/6573 [==============================] - 3s 476us/step - loss: 0.0487 - rmse: 0.3186 - val_loss: 0.0637 - val_rmse: 0.3722\n",
      "Epoch 116/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0503 - rmse: 0.3238 - val_loss: 0.0622 - val_rmse: 0.3681\n",
      "Epoch 117/1600\n",
      "6573/6573 [==============================] - 3s 471us/step - loss: 0.0499 - rmse: 0.3232 - val_loss: 0.0561 - val_rmse: 0.3495\n",
      "Epoch 118/1600\n",
      "6573/6573 [==============================] - 3s 454us/step - loss: 0.0499 - rmse: 0.3227 - val_loss: 0.0629 - val_rmse: 0.3696\n",
      "Epoch 119/1600\n",
      "6573/6573 [==============================] - 3s 445us/step - loss: 0.0502 - rmse: 0.3242 - val_loss: 0.0573 - val_rmse: 0.3526\n",
      "Epoch 120/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0521 - rmse: 0.34186573/6573 [==============================] - 3s 480us/step - loss: 0.0489 - rmse: 0.3187 - val_loss: 0.0572 - val_rmse: 0.3510\n",
      "Epoch 121/1600\n",
      "6573/6573 [==============================] - 3s 479us/step - loss: 0.0481 - rmse: 0.3172 - val_loss: 0.0582 - val_rmse: 0.3549\n",
      "Epoch 122/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0482 - rmse: 0.3166 - val_loss: 0.0559 - val_rmse: 0.3485\n",
      "Epoch 123/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0497 - rmse: 0.3228 - val_loss: 0.0581 - val_rmse: 0.3561\n",
      "Epoch 124/1600\n",
      "6573/6573 [==============================] - 3s 483us/step - loss: 0.0491 - rmse: 0.3196 - val_loss: 0.0615 - val_rmse: 0.3643\n",
      "Epoch 125/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0491 - rmse: 0.31896573/6573 [==============================] - 3s 450us/step - loss: 0.0486 - rmse: 0.3181 - val_loss: 0.0553 - val_rmse: 0.3473\n",
      "Epoch 126/1600\n",
      "6573/6573 [==============================] - 3s 458us/step - loss: 0.0480 - rmse: 0.3166 - val_loss: 0.0559 - val_rmse: 0.3480\n",
      "Epoch 127/1600\n",
      "6573/6573 [==============================] - 3s 486us/step - loss: 0.0472 - rmse: 0.3140 - val_loss: 0.0541 - val_rmse: 0.3430\n",
      "Epoch 128/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0477 - rmse: 0.3146 - val_loss: 0.0571 - val_rmse: 0.3516\n",
      "Epoch 129/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0481 - rmse: 0.3173 - val_loss: 0.0618 - val_rmse: 0.3666\n",
      "Epoch 130/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0432 - rmse: 0.29836573/6573 [==============================] - 3s 472us/step - loss: 0.0457 - rmse: 0.3074 - val_loss: 0.0547 - val_rmse: 0.3443\n",
      "Epoch 131/1600\n",
      "6573/6573 [==============================] - 3s 464us/step - loss: 0.0472 - rmse: 0.3135 - val_loss: 0.0694 - val_rmse: 0.3911\n",
      "Epoch 132/1600\n",
      "6573/6573 [==============================] - 3s 443us/step - loss: 0.0468 - rmse: 0.3123 - val_loss: 0.0529 - val_rmse: 0.3384\n",
      "Epoch 133/1600\n",
      "6573/6573 [==============================] - 3s 456us/step - loss: 0.0477 - rmse: 0.3147 - val_loss: 0.0568 - val_rmse: 0.3506\n",
      "Epoch 134/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0476 - rmse: 0.3149 - val_loss: 0.0559 - val_rmse: 0.3473\n",
      "Epoch 135/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0344 - rmse: 0.26576573/6573 [==============================] - 3s 470us/step - loss: 0.0464 - rmse: 0.3102 - val_loss: 0.0547 - val_rmse: 0.3444\n",
      "Epoch 136/1600\n",
      "6573/6573 [==============================] - 3s 469us/step - loss: 0.0470 - rmse: 0.3125 - val_loss: 0.0547 - val_rmse: 0.3444\n",
      "Epoch 137/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0471 - rmse: 0.3127 - val_loss: 0.0559 - val_rmse: 0.3482\n",
      "Epoch 138/1600\n",
      "6573/6573 [==============================] - 3s 459us/step - loss: 0.0471 - rmse: 0.3131 - val_loss: 0.0601 - val_rmse: 0.3608\n",
      "Epoch 139/1600\n",
      "6573/6573 [==============================] - 3s 448us/step - loss: 0.0471 - rmse: 0.3133 - val_loss: 0.0543 - val_rmse: 0.3436\n",
      "Epoch 140/1600\n",
      "  32/6573 [..............................] - ETA: 4s - loss: 0.0394 - rmse: 0.28896573/6573 [==============================] - 3s 464us/step - loss: 0.0455 - rmse: 0.3074 - val_loss: 0.0568 - val_rmse: 0.3507\n",
      "Epoch 141/1600\n",
      "6573/6573 [==============================] - 3s 482us/step - loss: 0.0460 - rmse: 0.3091 - val_loss: 0.0589 - val_rmse: 0.3586\n",
      "Epoch 142/1600\n",
      "6573/6573 [==============================] - 3s 477us/step - loss: 0.0450 - rmse: 0.3055 - val_loss: 0.0635 - val_rmse: 0.3705\n",
      "Epoch 143/1600\n",
      "6573/6573 [==============================] - 3s 471us/step - loss: 0.0455 - rmse: 0.3073 - val_loss: 0.0537 - val_rmse: 0.3416\n",
      "Epoch 144/1600\n",
      "6573/6573 [==============================] - 3s 482us/step - loss: 0.0444 - rmse: 0.3031 - val_loss: 0.0544 - val_rmse: 0.3441\n",
      "Epoch 145/1600\n",
      " 288/6573 [>.............................] - ETA: 3s - loss: 0.0505 - rmse: 0.32606573/6573 [==============================] - 3s 458us/step - loss: 0.0462 - rmse: 0.3102 - val_loss: 0.0580 - val_rmse: 0.3540\n",
      "Epoch 146/1600\n",
      "6573/6573 [==============================] - 3s 444us/step - loss: 0.0449 - rmse: 0.3058 - val_loss: 0.0584 - val_rmse: 0.3558\n",
      "Epoch 147/1600\n",
      "6573/6573 [==============================] - 3s 476us/step - loss: 0.0456 - rmse: 0.3076 - val_loss: 0.0557 - val_rmse: 0.3469\n",
      "Epoch 148/1600\n",
      "6573/6573 [==============================] - 3s 476us/step - loss: 0.0456 - rmse: 0.3083 - val_loss: 0.0576 - val_rmse: 0.3522\n",
      "Epoch 149/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0450 - rmse: 0.3052 - val_loss: 0.0553 - val_rmse: 0.3457\n",
      "Epoch 150/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0331 - rmse: 0.26116573/6573 [==============================] - 3s 471us/step - loss: 0.0450 - rmse: 0.3059 - val_loss: 0.0537 - val_rmse: 0.3405\n",
      "Epoch 151/1600\n",
      "6573/6573 [==============================] - 3s 483us/step - loss: 0.0450 - rmse: 0.3058 - val_loss: 0.0558 - val_rmse: 0.3471\n",
      "Epoch 152/1600\n",
      "6573/6573 [==============================] - 3s 448us/step - loss: 0.0432 - rmse: 0.2996 - val_loss: 0.0582 - val_rmse: 0.3556\n",
      "Epoch 153/1600\n",
      "6573/6573 [==============================] - 3s 452us/step - loss: 0.0434 - rmse: 0.2994 - val_loss: 0.0534 - val_rmse: 0.3403\n",
      "Epoch 154/1600\n",
      "6573/6573 [==============================] - 3s 479us/step - loss: 0.0439 - rmse: 0.3014 - val_loss: 0.0534 - val_rmse: 0.3395\n",
      "Epoch 155/1600\n",
      " 256/6573 [>.............................] - ETA: 3s - loss: 0.0516 - rmse: 0.32826573/6573 [==============================] - 3s 473us/step - loss: 0.0430 - rmse: 0.2989 - val_loss: 0.0525 - val_rmse: 0.3358\n",
      "Epoch 156/1600\n",
      "6573/6573 [==============================] - 3s 481us/step - loss: 0.0435 - rmse: 0.3000 - val_loss: 0.0511 - val_rmse: 0.3325\n",
      "Epoch 157/1600\n",
      "6573/6573 [==============================] - 3s 471us/step - loss: 0.0435 - rmse: 0.3002 - val_loss: 0.0570 - val_rmse: 0.3517\n",
      "Epoch 158/1600\n",
      "6573/6573 [==============================] - 3s 466us/step - loss: 0.0445 - rmse: 0.3039 - val_loss: 0.0518 - val_rmse: 0.3342\n",
      "Epoch 159/1600\n",
      "6573/6573 [==============================] - 3s 443us/step - loss: 0.0442 - rmse: 0.3031 - val_loss: 0.0530 - val_rmse: 0.3385\n",
      "Epoch 160/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0492 - rmse: 0.32086573/6573 [==============================] - 3s 463us/step - loss: 0.0440 - rmse: 0.3026 - val_loss: 0.0567 - val_rmse: 0.3500\n",
      "Epoch 161/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0425 - rmse: 0.2966 - val_loss: 0.0534 - val_rmse: 0.3398\n",
      "Epoch 162/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0434 - rmse: 0.3000 - val_loss: 0.0610 - val_rmse: 0.3636\n",
      "Epoch 163/1600\n",
      "6573/6573 [==============================] - 3s 482us/step - loss: 0.0431 - rmse: 0.2990 - val_loss: 0.0540 - val_rmse: 0.3416\n",
      "Epoch 164/1600\n",
      "6573/6573 [==============================] - 3s 484us/step - loss: 0.0432 - rmse: 0.2995 - val_loss: 0.0537 - val_rmse: 0.3414\n",
      "Epoch 165/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0526 - rmse: 0.33046573/6573 [==============================] - 3s 467us/step - loss: 0.0435 - rmse: 0.2997 - val_loss: 0.0528 - val_rmse: 0.3382\n",
      "Epoch 166/1600\n",
      "6573/6573 [==============================] - 3s 443us/step - loss: 0.0424 - rmse: 0.2973 - val_loss: 0.0533 - val_rmse: 0.3399\n",
      "Epoch 167/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0431 - rmse: 0.2986 - val_loss: 0.0513 - val_rmse: 0.3334\n",
      "Epoch 168/1600\n",
      "6573/6573 [==============================] - 3s 482us/step - loss: 0.0432 - rmse: 0.2984 - val_loss: 0.0513 - val_rmse: 0.3333\n",
      "Epoch 169/1600\n",
      "6573/6573 [==============================] - 3s 476us/step - loss: 0.0432 - rmse: 0.2993 - val_loss: 0.0517 - val_rmse: 0.3338\n",
      "Epoch 170/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0510 - rmse: 0.32616573/6573 [==============================] - 3s 471us/step - loss: 0.0432 - rmse: 0.2988 - val_loss: 0.0545 - val_rmse: 0.3434\n",
      "Epoch 171/1600\n",
      "6573/6573 [==============================] - 3s 483us/step - loss: 0.0442 - rmse: 0.3020 - val_loss: 0.0515 - val_rmse: 0.3335\n",
      "Epoch 172/1600\n",
      "6573/6573 [==============================] - 3s 460us/step - loss: 0.0434 - rmse: 0.3004 - val_loss: 0.0585 - val_rmse: 0.3552\n",
      "Epoch 173/1600\n",
      "6573/6573 [==============================] - 3s 454us/step - loss: 0.0423 - rmse: 0.2956 - val_loss: 0.0549 - val_rmse: 0.3441\n",
      "Epoch 174/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0413 - rmse: 0.2927 - val_loss: 0.0504 - val_rmse: 0.3308\n",
      "Epoch 175/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0353 - rmse: 0.26826573/6573 [==============================] - 3s 472us/step - loss: 0.0421 - rmse: 0.2945 - val_loss: 0.0507 - val_rmse: 0.3305\n",
      "Epoch 176/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0422 - rmse: 0.2959 - val_loss: 0.0533 - val_rmse: 0.3389\n",
      "Epoch 177/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0432 - rmse: 0.2984 - val_loss: 0.0535 - val_rmse: 0.3401\n",
      "Epoch 178/1600\n",
      "6573/6573 [==============================] - 3s 483us/step - loss: 0.0425 - rmse: 0.2973 - val_loss: 0.0501 - val_rmse: 0.3296\n",
      "Epoch 179/1600\n",
      "6573/6573 [==============================] - 3s 455us/step - loss: 0.0419 - rmse: 0.2941 - val_loss: 0.0537 - val_rmse: 0.3409\n",
      "Epoch 180/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0471 - rmse: 0.31556573/6573 [==============================] - 3s 470us/step - loss: 0.0418 - rmse: 0.2936 - val_loss: 0.0519 - val_rmse: 0.3348\n",
      "Epoch 181/1600\n",
      "6573/6573 [==============================] - 3s 486us/step - loss: 0.0410 - rmse: 0.2905 - val_loss: 0.0553 - val_rmse: 0.3454\n",
      "Epoch 182/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0415 - rmse: 0.2932 - val_loss: 0.0531 - val_rmse: 0.3379\n",
      "Epoch 183/1600\n",
      "6573/6573 [==============================] - 3s 484us/step - loss: 0.0415 - rmse: 0.2925 - val_loss: 0.0500 - val_rmse: 0.3294\n",
      "Epoch 184/1600\n",
      "6573/6573 [==============================] - 3s 481us/step - loss: 0.0412 - rmse: 0.2914 - val_loss: 0.0521 - val_rmse: 0.3361\n",
      "Epoch 185/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0343 - rmse: 0.26636573/6573 [==============================] - 3s 469us/step - loss: 0.0412 - rmse: 0.2915 - val_loss: 0.0505 - val_rmse: 0.3313\n",
      "Epoch 186/1600\n",
      "6573/6573 [==============================] - 3s 448us/step - loss: 0.0411 - rmse: 0.2911 - val_loss: 0.0505 - val_rmse: 0.3305\n",
      "Epoch 187/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0414 - rmse: 0.2930 - val_loss: 0.0493 - val_rmse: 0.3262\n",
      "Epoch 188/1600\n",
      "6573/6573 [==============================] - 3s 476us/step - loss: 0.0397 - rmse: 0.2864 - val_loss: 0.0563 - val_rmse: 0.3492\n",
      "Epoch 189/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0403 - rmse: 0.2892 - val_loss: 0.0502 - val_rmse: 0.3292\n",
      "Epoch 190/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0601 - rmse: 0.35986573/6573 [==============================] - 3s 473us/step - loss: 0.0408 - rmse: 0.2890 - val_loss: 0.0503 - val_rmse: 0.3296\n",
      "Epoch 191/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0413 - rmse: 0.2927 - val_loss: 0.0509 - val_rmse: 0.3315\n",
      "Epoch 192/1600\n",
      "6573/6573 [==============================] - 3s 460us/step - loss: 0.0412 - rmse: 0.2919 - val_loss: 0.0510 - val_rmse: 0.3315\n",
      "Epoch 193/1600\n",
      "6573/6573 [==============================] - 3s 451us/step - loss: 0.0395 - rmse: 0.2848 - val_loss: 0.0507 - val_rmse: 0.3310\n",
      "Epoch 194/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0404 - rmse: 0.2891 - val_loss: 0.0514 - val_rmse: 0.3331\n",
      "Epoch 195/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0390 - rmse: 0.28756573/6573 [==============================] - 3s 474us/step - loss: 0.0417 - rmse: 0.2931 - val_loss: 0.0537 - val_rmse: 0.3408\n",
      "Epoch 196/1600\n",
      "6573/6573 [==============================] - 3s 476us/step - loss: 0.0398 - rmse: 0.2867 - val_loss: 0.0510 - val_rmse: 0.3323\n",
      "Epoch 197/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0395 - rmse: 0.2846 - val_loss: 0.0525 - val_rmse: 0.3356\n",
      "Epoch 198/1600\n",
      "6573/6573 [==============================] - 3s 477us/step - loss: 0.0400 - rmse: 0.2873 - val_loss: 0.0518 - val_rmse: 0.3338\n",
      "Epoch 199/1600\n",
      "6573/6573 [==============================] - 3s 450us/step - loss: 0.0396 - rmse: 0.2858 - val_loss: 0.0538 - val_rmse: 0.3413\n",
      "Epoch 200/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0500 - rmse: 0.32926573/6573 [==============================] - 3s 454us/step - loss: 0.0387 - rmse: 0.2822 - val_loss: 0.0533 - val_rmse: 0.3389\n",
      "Epoch 201/1600\n",
      "6573/6573 [==============================] - 3s 481us/step - loss: 0.0405 - rmse: 0.2890 - val_loss: 0.0499 - val_rmse: 0.3279\n",
      "Epoch 202/1600\n",
      "6573/6573 [==============================] - 3s 488us/step - loss: 0.0391 - rmse: 0.2841 - val_loss: 0.0529 - val_rmse: 0.3375\n",
      "Epoch 203/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0398 - rmse: 0.2865 - val_loss: 0.0512 - val_rmse: 0.3326\n",
      "Epoch 204/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0394 - rmse: 0.2847 - val_loss: 0.0541 - val_rmse: 0.3428\n",
      "Epoch 205/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0494 - rmse: 0.32106573/6573 [==============================] - 3s 467us/step - loss: 0.0400 - rmse: 0.2874 - val_loss: 0.0527 - val_rmse: 0.3364\n",
      "Epoch 206/1600\n",
      "6573/6573 [==============================] - 3s 436us/step - loss: 0.0405 - rmse: 0.2896 - val_loss: 0.0531 - val_rmse: 0.3381\n",
      "Epoch 207/1600\n",
      "6573/6573 [==============================] - 3s 460us/step - loss: 0.0403 - rmse: 0.2874 - val_loss: 0.0520 - val_rmse: 0.3361\n",
      "Epoch 208/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0387 - rmse: 0.2823 - val_loss: 0.0501 - val_rmse: 0.3303\n",
      "Epoch 209/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0396 - rmse: 0.2857 - val_loss: 0.0489 - val_rmse: 0.3251\n",
      "Epoch 210/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0434 - rmse: 0.30546573/6573 [==============================] - 3s 472us/step - loss: 0.0392 - rmse: 0.2833 - val_loss: 0.0515 - val_rmse: 0.3335\n",
      "Epoch 211/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.0388 - rmse: 0.2825 - val_loss: 0.0504 - val_rmse: 0.3296\n",
      "Epoch 212/1600\n",
      "6573/6573 [==============================] - 3s 465us/step - loss: 0.0386 - rmse: 0.2817 - val_loss: 0.0604 - val_rmse: 0.3625\n",
      "Epoch 213/1600\n",
      "6573/6573 [==============================] - 3s 436us/step - loss: 0.0384 - rmse: 0.2811 - val_loss: 0.0485 - val_rmse: 0.3238\n",
      "Epoch 214/1600\n",
      "6573/6573 [==============================] - 3s 464us/step - loss: 0.0385 - rmse: 0.2815 - val_loss: 0.0545 - val_rmse: 0.3423\n",
      "Epoch 215/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0418 - rmse: 0.29296573/6573 [==============================] - 3s 476us/step - loss: 0.0389 - rmse: 0.2824 - val_loss: 0.0506 - val_rmse: 0.3301\n",
      "Epoch 216/1600\n",
      "6573/6573 [==============================] - 3s 481us/step - loss: 0.0374 - rmse: 0.2772 - val_loss: 0.0505 - val_rmse: 0.3305\n",
      "Epoch 217/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0384 - rmse: 0.2814 - val_loss: 0.0500 - val_rmse: 0.3280\n",
      "Epoch 218/1600\n",
      "6573/6573 [==============================] - 3s 489us/step - loss: 0.0393 - rmse: 0.2848 - val_loss: 0.0511 - val_rmse: 0.3320\n",
      "Epoch 219/1600\n",
      "6573/6573 [==============================] - 3s 467us/step - loss: 0.0392 - rmse: 0.2840 - val_loss: 0.0498 - val_rmse: 0.3270\n",
      "Epoch 220/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0346 - rmse: 0.26706573/6573 [==============================] - 3s 455us/step - loss: 0.0382 - rmse: 0.2806 - val_loss: 0.0527 - val_rmse: 0.3367\n",
      "Epoch 221/1600\n",
      "6573/6573 [==============================] - 3s 484us/step - loss: 0.0389 - rmse: 0.2835 - val_loss: 0.0584 - val_rmse: 0.3551\n",
      "Epoch 222/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0392 - rmse: 0.2847 - val_loss: 0.0508 - val_rmse: 0.3305\n",
      "Epoch 223/1600\n",
      "6573/6573 [==============================] - 3s 482us/step - loss: 0.0377 - rmse: 0.2783 - val_loss: 0.0526 - val_rmse: 0.3366\n",
      "Epoch 224/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0386 - rmse: 0.2825 - val_loss: 0.0534 - val_rmse: 0.3394\n",
      "Epoch 225/1600\n",
      " 256/6573 [>.............................] - ETA: 3s - loss: 0.0357 - rmse: 0.26796573/6573 [==============================] - 3s 476us/step - loss: 0.0379 - rmse: 0.2790 - val_loss: 0.0501 - val_rmse: 0.3297\n",
      "Epoch 226/1600\n",
      "6573/6573 [==============================] - 3s 443us/step - loss: 0.0383 - rmse: 0.2808 - val_loss: 0.0497 - val_rmse: 0.3275\n",
      "Epoch 227/1600\n",
      "6573/6573 [==============================] - 3s 451us/step - loss: 0.0383 - rmse: 0.2810 - val_loss: 0.0486 - val_rmse: 0.3235\n",
      "Epoch 228/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0374 - rmse: 0.2766 - val_loss: 0.0490 - val_rmse: 0.3265\n",
      "Epoch 229/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0379 - rmse: 0.2793 - val_loss: 0.0501 - val_rmse: 0.3284\n",
      "Epoch 230/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0387 - rmse: 0.28436573/6573 [==============================] - 3s 470us/step - loss: 0.0373 - rmse: 0.2771 - val_loss: 0.0493 - val_rmse: 0.3258\n",
      "Epoch 231/1600\n",
      "6573/6573 [==============================] - 3s 467us/step - loss: 0.0379 - rmse: 0.2794 - val_loss: 0.0499 - val_rmse: 0.3285\n",
      "Epoch 232/1600\n",
      "6573/6573 [==============================] - 3s 465us/step - loss: 0.0373 - rmse: 0.2769 - val_loss: 0.0482 - val_rmse: 0.3225\n",
      "Epoch 233/1600\n",
      "6573/6573 [==============================] - 3s 430us/step - loss: 0.0373 - rmse: 0.2765 - val_loss: 0.0507 - val_rmse: 0.3311\n",
      "Epoch 234/1600\n",
      "6573/6573 [==============================] - 3s 457us/step - loss: 0.0381 - rmse: 0.2798 - val_loss: 0.0492 - val_rmse: 0.3255\n",
      "Epoch 235/1600\n",
      " 288/6573 [>.............................] - ETA: 3s - loss: 0.0379 - rmse: 0.27956573/6573 [==============================] - 3s 476us/step - loss: 0.0369 - rmse: 0.2753 - val_loss: 0.0496 - val_rmse: 0.3273\n",
      "Epoch 236/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0366 - rmse: 0.2736 - val_loss: 0.0492 - val_rmse: 0.3262\n",
      "Epoch 237/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0366 - rmse: 0.2744 - val_loss: 0.0488 - val_rmse: 0.3236\n",
      "Epoch 238/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0379 - rmse: 0.2794 - val_loss: 0.0525 - val_rmse: 0.3363\n",
      "Epoch 239/1600\n",
      "6573/6573 [==============================] - 3s 467us/step - loss: 0.0378 - rmse: 0.2786 - val_loss: 0.0515 - val_rmse: 0.3332\n",
      "Epoch 240/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0289 - rmse: 0.24236573/6573 [==============================] - 3s 438us/step - loss: 0.0360 - rmse: 0.2720 - val_loss: 0.0508 - val_rmse: 0.3313\n",
      "Epoch 241/1600\n",
      "6573/6573 [==============================] - 3s 467us/step - loss: 0.0371 - rmse: 0.2762 - val_loss: 0.0507 - val_rmse: 0.3315\n",
      "Epoch 242/1600\n",
      "6573/6573 [==============================] - 3s 486us/step - loss: 0.0365 - rmse: 0.2734 - val_loss: 0.0535 - val_rmse: 0.3394\n",
      "Epoch 243/1600\n",
      "6573/6573 [==============================] - 3s 492us/step - loss: 0.0368 - rmse: 0.2742 - val_loss: 0.0490 - val_rmse: 0.3259\n",
      "Epoch 244/1600\n",
      "6573/6573 [==============================] - 3s 479us/step - loss: 0.0366 - rmse: 0.2735 - val_loss: 0.0501 - val_rmse: 0.3283\n",
      "Epoch 245/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0371 - rmse: 0.27606573/6573 [==============================] - 3s 479us/step - loss: 0.0369 - rmse: 0.2752 - val_loss: 0.0506 - val_rmse: 0.3306\n",
      "Epoch 246/1600\n",
      "6573/6573 [==============================] - 3s 454us/step - loss: 0.0366 - rmse: 0.2740 - val_loss: 0.0493 - val_rmse: 0.3256\n",
      "Epoch 247/1600\n",
      "6573/6573 [==============================] - 3s 446us/step - loss: 0.0359 - rmse: 0.2715 - val_loss: 0.0476 - val_rmse: 0.3205\n",
      "Epoch 248/1600\n",
      "6573/6573 [==============================] - 3s 471us/step - loss: 0.0355 - rmse: 0.2704 - val_loss: 0.0479 - val_rmse: 0.3211\n",
      "Epoch 249/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0361 - rmse: 0.2721 - val_loss: 0.0491 - val_rmse: 0.3251\n",
      "Epoch 250/1600\n",
      " 160/6573 [..............................] - ETA: 2s - loss: 0.0290 - rmse: 0.24406573/6573 [==============================] - 3s 468us/step - loss: 0.0376 - rmse: 0.2781 - val_loss: 0.0471 - val_rmse: 0.3192\n",
      "Epoch 251/1600\n",
      "6573/6573 [==============================] - 3s 467us/step - loss: 0.0356 - rmse: 0.2701 - val_loss: 0.0477 - val_rmse: 0.3214\n",
      "Epoch 252/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0364 - rmse: 0.2733 - val_loss: 0.0506 - val_rmse: 0.3302\n",
      "Epoch 253/1600\n",
      "6573/6573 [==============================] - 3s 441us/step - loss: 0.0372 - rmse: 0.2772 - val_loss: 0.0481 - val_rmse: 0.3219\n",
      "Epoch 254/1600\n",
      "6573/6573 [==============================] - 3s 448us/step - loss: 0.0363 - rmse: 0.2729 - val_loss: 0.0480 - val_rmse: 0.3216\n",
      "Epoch 255/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0344 - rmse: 0.26626573/6573 [==============================] - 3s 472us/step - loss: 0.0367 - rmse: 0.2748 - val_loss: 0.0503 - val_rmse: 0.3283\n",
      "Epoch 256/1600\n",
      "6573/6573 [==============================] - 3s 477us/step - loss: 0.0348 - rmse: 0.2668 - val_loss: 0.0502 - val_rmse: 0.3287\n",
      "Epoch 257/1600\n",
      "6573/6573 [==============================] - 3s 490us/step - loss: 0.0364 - rmse: 0.2733 - val_loss: 0.0486 - val_rmse: 0.3233\n",
      "Epoch 258/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0359 - rmse: 0.2707 - val_loss: 0.0469 - val_rmse: 0.3181\n",
      "Epoch 259/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0356 - rmse: 0.2703 - val_loss: 0.0478 - val_rmse: 0.3205\n",
      "Epoch 260/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0365 - rmse: 0.27436573/6573 [==============================] - 3s 431us/step - loss: 0.0355 - rmse: 0.2698 - val_loss: 0.0482 - val_rmse: 0.3220\n",
      "Epoch 261/1600\n",
      "6573/6573 [==============================] - 3s 463us/step - loss: 0.0362 - rmse: 0.2730 - val_loss: 0.0538 - val_rmse: 0.3409\n",
      "Epoch 262/1600\n",
      "6573/6573 [==============================] - 3s 479us/step - loss: 0.0362 - rmse: 0.2719 - val_loss: 0.0524 - val_rmse: 0.3360\n",
      "Epoch 263/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0351 - rmse: 0.2688 - val_loss: 0.0468 - val_rmse: 0.3178\n",
      "Epoch 264/1600\n",
      "6573/6573 [==============================] - 3s 469us/step - loss: 0.0355 - rmse: 0.2698 - val_loss: 0.0486 - val_rmse: 0.3225\n",
      "Epoch 265/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0366 - rmse: 0.27536573/6573 [==============================] - 3s 473us/step - loss: 0.0359 - rmse: 0.2715 - val_loss: 0.0489 - val_rmse: 0.3239\n",
      "Epoch 266/1600\n",
      "6573/6573 [==============================] - 3s 461us/step - loss: 0.0351 - rmse: 0.2680 - val_loss: 0.0497 - val_rmse: 0.3275\n",
      "Epoch 267/1600\n",
      "6573/6573 [==============================] - 3s 438us/step - loss: 0.0357 - rmse: 0.2703 - val_loss: 0.0495 - val_rmse: 0.3261\n",
      "Epoch 268/1600\n",
      "6573/6573 [==============================] - 3s 467us/step - loss: 0.0363 - rmse: 0.2729 - val_loss: 0.0504 - val_rmse: 0.3299\n",
      "Epoch 269/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0364 - rmse: 0.2740 - val_loss: 0.0527 - val_rmse: 0.3364\n",
      "Epoch 270/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0409 - rmse: 0.29146573/6573 [==============================] - 3s 470us/step - loss: 0.0361 - rmse: 0.2721 - val_loss: 0.0499 - val_rmse: 0.3274\n",
      "Epoch 271/1600\n",
      "6573/6573 [==============================] - 3s 464us/step - loss: 0.0350 - rmse: 0.2673 - val_loss: 0.0474 - val_rmse: 0.3193\n",
      "Epoch 272/1600\n",
      "6573/6573 [==============================] - 3s 466us/step - loss: 0.0350 - rmse: 0.2679 - val_loss: 0.0491 - val_rmse: 0.3250\n",
      "Epoch 273/1600\n",
      "6573/6573 [==============================] - 3s 453us/step - loss: 0.0361 - rmse: 0.2719 - val_loss: 0.0480 - val_rmse: 0.3211\n",
      "Epoch 274/1600\n",
      "6573/6573 [==============================] - 3s 438us/step - loss: 0.0339 - rmse: 0.2632 - val_loss: 0.0464 - val_rmse: 0.3154\n",
      "Epoch 275/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0311 - rmse: 0.25326573/6573 [==============================] - 3s 462us/step - loss: 0.0354 - rmse: 0.2694 - val_loss: 0.0470 - val_rmse: 0.3184\n",
      "Epoch 276/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0351 - rmse: 0.2687 - val_loss: 0.0505 - val_rmse: 0.3295\n",
      "Epoch 277/1600\n",
      "6573/6573 [==============================] - 3s 471us/step - loss: 0.0354 - rmse: 0.2696 - val_loss: 0.0482 - val_rmse: 0.3223\n",
      "Epoch 278/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0349 - rmse: 0.2669 - val_loss: 0.0478 - val_rmse: 0.3217\n",
      "Epoch 279/1600\n",
      "6573/6573 [==============================] - 3s 469us/step - loss: 0.0347 - rmse: 0.2669 - val_loss: 0.0473 - val_rmse: 0.3188\n",
      "Epoch 280/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0345 - rmse: 0.26846573/6573 [==============================] - 3s 453us/step - loss: 0.0356 - rmse: 0.2699 - val_loss: 0.0534 - val_rmse: 0.3393\n",
      "Epoch 281/1600\n",
      "6573/6573 [==============================] - 3s 443us/step - loss: 0.0358 - rmse: 0.2717 - val_loss: 0.0472 - val_rmse: 0.3182\n",
      "Epoch 282/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0343 - rmse: 0.2648 - val_loss: 0.0486 - val_rmse: 0.3244\n",
      "Epoch 283/1600\n",
      "6573/6573 [==============================] - 3s 479us/step - loss: 0.0344 - rmse: 0.2650 - val_loss: 0.0480 - val_rmse: 0.3216\n",
      "Epoch 284/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0351 - rmse: 0.2687 - val_loss: 0.0502 - val_rmse: 0.3284\n",
      "Epoch 285/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0377 - rmse: 0.27976573/6573 [==============================] - 3s 461us/step - loss: 0.0349 - rmse: 0.2670 - val_loss: 0.0478 - val_rmse: 0.3204\n",
      "Epoch 286/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0346 - rmse: 0.2667 - val_loss: 0.0469 - val_rmse: 0.3175\n",
      "Epoch 287/1600\n",
      "6573/6573 [==============================] - 3s 444us/step - loss: 0.0342 - rmse: 0.2646 - val_loss: 0.0467 - val_rmse: 0.3170\n",
      "Epoch 288/1600\n",
      "6573/6573 [==============================] - 3s 442us/step - loss: 0.0339 - rmse: 0.2632 - val_loss: 0.0499 - val_rmse: 0.3281\n",
      "Epoch 289/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0346 - rmse: 0.2666 - val_loss: 0.0495 - val_rmse: 0.3268\n",
      "Epoch 290/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0295 - rmse: 0.24616573/6573 [==============================] - 3s 464us/step - loss: 0.0340 - rmse: 0.2639 - val_loss: 0.0481 - val_rmse: 0.3221\n",
      "Epoch 291/1600\n",
      "6573/6573 [==============================] - 3s 460us/step - loss: 0.0342 - rmse: 0.2647 - val_loss: 0.0467 - val_rmse: 0.3168\n",
      "Epoch 292/1600\n",
      "6573/6573 [==============================] - 3s 464us/step - loss: 0.0345 - rmse: 0.2660 - val_loss: 0.0480 - val_rmse: 0.3225\n",
      "Epoch 293/1600\n",
      "6573/6573 [==============================] - 3s 467us/step - loss: 0.0350 - rmse: 0.2675 - val_loss: 0.0457 - val_rmse: 0.3138\n",
      "Epoch 294/1600\n",
      "6573/6573 [==============================] - 3s 437us/step - loss: 0.0343 - rmse: 0.2650 - val_loss: 0.0474 - val_rmse: 0.3191\n",
      "Epoch 295/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0357 - rmse: 0.27306573/6573 [==============================] - 3s 454us/step - loss: 0.0346 - rmse: 0.2657 - val_loss: 0.0488 - val_rmse: 0.3234\n",
      "Epoch 296/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0337 - rmse: 0.2629 - val_loss: 0.0482 - val_rmse: 0.3221\n",
      "Epoch 297/1600\n",
      "6573/6573 [==============================] - 3s 481us/step - loss: 0.0339 - rmse: 0.2636 - val_loss: 0.0473 - val_rmse: 0.3195\n",
      "Epoch 298/1600\n",
      "6573/6573 [==============================] - 3s 481us/step - loss: 0.0340 - rmse: 0.2636 - val_loss: 0.0457 - val_rmse: 0.3132\n",
      "Epoch 299/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0335 - rmse: 0.2618 - val_loss: 0.0488 - val_rmse: 0.3245\n",
      "Epoch 300/1600\n",
      " 288/6573 [>.............................] - ETA: 3s - loss: 0.0280 - rmse: 0.23886573/6573 [==============================] - 3s 470us/step - loss: 0.0338 - rmse: 0.2620 - val_loss: 0.0494 - val_rmse: 0.3262\n",
      "Epoch 301/1600\n",
      "6573/6573 [==============================] - 3s 436us/step - loss: 0.0346 - rmse: 0.2667 - val_loss: 0.0482 - val_rmse: 0.3223\n",
      "Epoch 302/1600\n",
      "6573/6573 [==============================] - 3s 456us/step - loss: 0.0349 - rmse: 0.2677 - val_loss: 0.0459 - val_rmse: 0.3144\n",
      "Epoch 303/1600\n",
      "6573/6573 [==============================] - 3s 479us/step - loss: 0.0337 - rmse: 0.2621 - val_loss: 0.0471 - val_rmse: 0.3182\n",
      "Epoch 304/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0336 - rmse: 0.2620 - val_loss: 0.0466 - val_rmse: 0.3173\n",
      "Epoch 305/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0344 - rmse: 0.26596573/6573 [==============================] - 3s 469us/step - loss: 0.0334 - rmse: 0.2614 - val_loss: 0.0466 - val_rmse: 0.3167\n",
      "Epoch 306/1600\n",
      "6573/6573 [==============================] - 3s 464us/step - loss: 0.0334 - rmse: 0.2607 - val_loss: 0.0480 - val_rmse: 0.3224\n",
      "Epoch 307/1600\n",
      "6573/6573 [==============================] - 3s 466us/step - loss: 0.0335 - rmse: 0.2618 - val_loss: 0.0474 - val_rmse: 0.3190\n",
      "Epoch 308/1600\n",
      "6573/6573 [==============================] - 3s 434us/step - loss: 0.0328 - rmse: 0.2591 - val_loss: 0.0495 - val_rmse: 0.3274\n",
      "Epoch 309/1600\n",
      "6573/6573 [==============================] - 3s 458us/step - loss: 0.0341 - rmse: 0.2632 - val_loss: 0.0459 - val_rmse: 0.3143\n",
      "Epoch 310/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0327 - rmse: 0.25986573/6573 [==============================] - 3s 467us/step - loss: 0.0332 - rmse: 0.2606 - val_loss: 0.0465 - val_rmse: 0.3163\n",
      "Epoch 311/1600\n",
      "6573/6573 [==============================] - 3s 465us/step - loss: 0.0328 - rmse: 0.2580 - val_loss: 0.0468 - val_rmse: 0.3176\n",
      "Epoch 312/1600\n",
      "6573/6573 [==============================] - 3s 460us/step - loss: 0.0336 - rmse: 0.2626 - val_loss: 0.0485 - val_rmse: 0.3241\n",
      "Epoch 313/1600\n",
      "6573/6573 [==============================] - 3s 460us/step - loss: 0.0334 - rmse: 0.2609 - val_loss: 0.0477 - val_rmse: 0.3200\n",
      "Epoch 314/1600\n",
      "6573/6573 [==============================] - 3s 446us/step - loss: 0.0324 - rmse: 0.2569 - val_loss: 0.0462 - val_rmse: 0.3153\n",
      "Epoch 315/1600\n",
      " 160/6573 [..............................] - ETA: 2s - loss: 0.0318 - rmse: 0.25576573/6573 [==============================] - 3s 438us/step - loss: 0.0329 - rmse: 0.2594 - val_loss: 0.0524 - val_rmse: 0.3361\n",
      "Epoch 316/1600\n",
      "6573/6573 [==============================] - 3s 467us/step - loss: 0.0326 - rmse: 0.2578 - val_loss: 0.0467 - val_rmse: 0.3175\n",
      "Epoch 317/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0330 - rmse: 0.2599 - val_loss: 0.0490 - val_rmse: 0.3239\n",
      "Epoch 318/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0324 - rmse: 0.2568 - val_loss: 0.0487 - val_rmse: 0.3231\n",
      "Epoch 319/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0325 - rmse: 0.2575 - val_loss: 0.0462 - val_rmse: 0.3154\n",
      "Epoch 320/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0268 - rmse: 0.23296573/6573 [==============================] - 3s 466us/step - loss: 0.0333 - rmse: 0.2614 - val_loss: 0.0470 - val_rmse: 0.3169\n",
      "Epoch 321/1600\n",
      "6573/6573 [==============================] - 3s 450us/step - loss: 0.0325 - rmse: 0.2579 - val_loss: 0.0461 - val_rmse: 0.3147\n",
      "Epoch 322/1600\n",
      "6573/6573 [==============================] - 3s 438us/step - loss: 0.0326 - rmse: 0.2578 - val_loss: 0.0521 - val_rmse: 0.3344\n",
      "Epoch 323/1600\n",
      "6573/6573 [==============================] - 3s 469us/step - loss: 0.0324 - rmse: 0.2566 - val_loss: 0.0468 - val_rmse: 0.3170\n",
      "Epoch 324/1600\n",
      "6573/6573 [==============================] - 3s 469us/step - loss: 0.0331 - rmse: 0.2597 - val_loss: 0.0471 - val_rmse: 0.3190\n",
      "Epoch 325/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0283 - rmse: 0.23936573/6573 [==============================] - 3s 462us/step - loss: 0.0323 - rmse: 0.2564 - val_loss: 0.0462 - val_rmse: 0.3141\n",
      "Epoch 326/1600\n",
      "6573/6573 [==============================] - 3s 462us/step - loss: 0.0321 - rmse: 0.2558 - val_loss: 0.0456 - val_rmse: 0.3127\n",
      "Epoch 327/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0314 - rmse: 0.2527 - val_loss: 0.0480 - val_rmse: 0.3209\n",
      "Epoch 328/1600\n",
      "6573/6573 [==============================] - 3s 446us/step - loss: 0.0325 - rmse: 0.2571 - val_loss: 0.0462 - val_rmse: 0.3146\n",
      "Epoch 329/1600\n",
      "6573/6573 [==============================] - 3s 435us/step - loss: 0.0330 - rmse: 0.2593 - val_loss: 0.0454 - val_rmse: 0.3119\n",
      "Epoch 330/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0387 - rmse: 0.28506573/6573 [==============================] - 3s 481us/step - loss: 0.0315 - rmse: 0.2529 - val_loss: 0.0456 - val_rmse: 0.3132\n",
      "Epoch 331/1600\n",
      "6573/6573 [==============================] - 3s 471us/step - loss: 0.0325 - rmse: 0.2575 - val_loss: 0.0458 - val_rmse: 0.3139\n",
      "Epoch 332/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.0325 - rmse: 0.2577 - val_loss: 0.0488 - val_rmse: 0.3249\n",
      "Epoch 333/1600\n",
      "6573/6573 [==============================] - 3s 459us/step - loss: 0.0325 - rmse: 0.2576 - val_loss: 0.0510 - val_rmse: 0.3315\n",
      "Epoch 334/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.0320 - rmse: 0.2551 - val_loss: 0.0510 - val_rmse: 0.3316\n",
      "Epoch 335/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0286 - rmse: 0.24136573/6573 [==============================] - 3s 449us/step - loss: 0.0317 - rmse: 0.2540 - val_loss: 0.0480 - val_rmse: 0.3205\n",
      "Epoch 336/1600\n",
      "6573/6573 [==============================] - 3s 450us/step - loss: 0.0322 - rmse: 0.2566 - val_loss: 0.0452 - val_rmse: 0.3115\n",
      "Epoch 337/1600\n",
      "6573/6573 [==============================] - 3s 482us/step - loss: 0.0313 - rmse: 0.2526 - val_loss: 0.0463 - val_rmse: 0.3150\n",
      "Epoch 338/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0312 - rmse: 0.2525 - val_loss: 0.0460 - val_rmse: 0.3148\n",
      "Epoch 339/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.0313 - rmse: 0.2526 - val_loss: 0.0469 - val_rmse: 0.3168\n",
      "Epoch 340/1600\n",
      " 288/6573 [>.............................] - ETA: 3s - loss: 0.0301 - rmse: 0.24566573/6573 [==============================] - 3s 470us/step - loss: 0.0326 - rmse: 0.2581 - val_loss: 0.0464 - val_rmse: 0.3150\n",
      "Epoch 341/1600\n",
      "6573/6573 [==============================] - 3s 469us/step - loss: 0.0322 - rmse: 0.2560 - val_loss: 0.0471 - val_rmse: 0.3178\n",
      "Epoch 342/1600\n",
      "6573/6573 [==============================] - 3s 448us/step - loss: 0.0312 - rmse: 0.2528 - val_loss: 0.0452 - val_rmse: 0.3113\n",
      "Epoch 343/1600\n",
      "6573/6573 [==============================] - 3s 462us/step - loss: 0.0334 - rmse: 0.2613 - val_loss: 0.0465 - val_rmse: 0.3156\n",
      "Epoch 344/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.0322 - rmse: 0.2561 - val_loss: 0.0479 - val_rmse: 0.3216\n",
      "Epoch 345/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0306 - rmse: 0.24656573/6573 [==============================] - 3s 479us/step - loss: 0.0315 - rmse: 0.2538 - val_loss: 0.0449 - val_rmse: 0.3100\n",
      "Epoch 346/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0322 - rmse: 0.2559 - val_loss: 0.0465 - val_rmse: 0.3164\n",
      "Epoch 347/1600\n",
      "6573/6573 [==============================] - 3s 481us/step - loss: 0.0307 - rmse: 0.2495 - val_loss: 0.0492 - val_rmse: 0.3253\n",
      "Epoch 348/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0319 - rmse: 0.2556 - val_loss: 0.0462 - val_rmse: 0.3146\n",
      "Epoch 349/1600\n",
      "6573/6573 [==============================] - 3s 449us/step - loss: 0.0314 - rmse: 0.2524 - val_loss: 0.0533 - val_rmse: 0.3386\n",
      "Epoch 350/1600\n",
      " 160/6573 [..............................] - ETA: 2s - loss: 0.0328 - rmse: 0.26016573/6573 [==============================] - 3s 470us/step - loss: 0.0308 - rmse: 0.2509 - val_loss: 0.0460 - val_rmse: 0.3139\n",
      "Epoch 351/1600\n",
      "6573/6573 [==============================] - 3s 478us/step - loss: 0.0319 - rmse: 0.2550 - val_loss: 0.0467 - val_rmse: 0.3177\n",
      "Epoch 352/1600\n",
      "6573/6573 [==============================] - 3s 487us/step - loss: 0.0312 - rmse: 0.2524 - val_loss: 0.0496 - val_rmse: 0.3266\n",
      "Epoch 353/1600\n",
      "6573/6573 [==============================] - 3s 478us/step - loss: 0.0313 - rmse: 0.2527 - val_loss: 0.0470 - val_rmse: 0.3178\n",
      "Epoch 354/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0322 - rmse: 0.2557 - val_loss: 0.0458 - val_rmse: 0.3133\n",
      "Epoch 355/1600\n",
      " 288/6573 [>.............................] - ETA: 3s - loss: 0.0306 - rmse: 0.25026573/6573 [==============================] - 3s 467us/step - loss: 0.0318 - rmse: 0.2545 - val_loss: 0.0453 - val_rmse: 0.3118\n",
      "Epoch 356/1600\n",
      "6573/6573 [==============================] - 3s 453us/step - loss: 0.0313 - rmse: 0.2528 - val_loss: 0.0475 - val_rmse: 0.3192\n",
      "Epoch 357/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0316 - rmse: 0.2537 - val_loss: 0.0473 - val_rmse: 0.3184\n",
      "Epoch 358/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0306 - rmse: 0.2486 - val_loss: 0.0473 - val_rmse: 0.3189\n",
      "Epoch 359/1600\n",
      "6573/6573 [==============================] - 3s 486us/step - loss: 0.0315 - rmse: 0.2527 - val_loss: 0.0497 - val_rmse: 0.3270\n",
      "Epoch 360/1600\n",
      " 288/6573 [>.............................] - ETA: 3s - loss: 0.0270 - rmse: 0.23246573/6573 [==============================] - 3s 475us/step - loss: 0.0318 - rmse: 0.2544 - val_loss: 0.0462 - val_rmse: 0.3159\n",
      "Epoch 361/1600\n",
      "6573/6573 [==============================] - 3s 490us/step - loss: 0.0310 - rmse: 0.2506 - val_loss: 0.0479 - val_rmse: 0.3207\n",
      "Epoch 362/1600\n",
      "6573/6573 [==============================] - 3s 448us/step - loss: 0.0309 - rmse: 0.2509 - val_loss: 0.0450 - val_rmse: 0.3109\n",
      "Epoch 363/1600\n",
      "6573/6573 [==============================] - 3s 458us/step - loss: 0.0315 - rmse: 0.2535 - val_loss: 0.0496 - val_rmse: 0.3264\n",
      "Epoch 364/1600\n",
      "6573/6573 [==============================] - 3s 486us/step - loss: 0.0311 - rmse: 0.2516 - val_loss: 0.0476 - val_rmse: 0.3201\n",
      "Epoch 365/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0342 - rmse: 0.26516573/6573 [==============================] - 3s 476us/step - loss: 0.0305 - rmse: 0.2493 - val_loss: 0.0461 - val_rmse: 0.3146\n",
      "Epoch 366/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.0311 - rmse: 0.2509 - val_loss: 0.0471 - val_rmse: 0.3176\n",
      "Epoch 367/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0309 - rmse: 0.2502 - val_loss: 0.0489 - val_rmse: 0.3249\n",
      "Epoch 368/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0306 - rmse: 0.2497 - val_loss: 0.0463 - val_rmse: 0.3152\n",
      "Epoch 369/1600\n",
      "6573/6573 [==============================] - 3s 443us/step - loss: 0.0318 - rmse: 0.2549 - val_loss: 0.0449 - val_rmse: 0.3106\n",
      "Epoch 370/1600\n",
      " 160/6573 [..............................] - ETA: 2s - loss: 0.0306 - rmse: 0.25056573/6573 [==============================] - 3s 463us/step - loss: 0.0308 - rmse: 0.2501 - val_loss: 0.0470 - val_rmse: 0.3183\n",
      "Epoch 371/1600\n",
      "6573/6573 [==============================] - 3s 476us/step - loss: 0.0312 - rmse: 0.2524 - val_loss: 0.0455 - val_rmse: 0.3126\n",
      "Epoch 372/1600\n",
      "6573/6573 [==============================] - 3s 477us/step - loss: 0.0308 - rmse: 0.2501 - val_loss: 0.0504 - val_rmse: 0.3294\n",
      "Epoch 373/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0306 - rmse: 0.2496 - val_loss: 0.0467 - val_rmse: 0.3164\n",
      "Epoch 374/1600\n",
      "6573/6573 [==============================] - 3s 487us/step - loss: 0.0301 - rmse: 0.2476 - val_loss: 0.0464 - val_rmse: 0.3159\n",
      "Epoch 375/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0314 - rmse: 0.25186573/6573 [==============================] - 3s 465us/step - loss: 0.0301 - rmse: 0.2474 - val_loss: 0.0463 - val_rmse: 0.3157\n",
      "Epoch 376/1600\n",
      "6573/6573 [==============================] - 3s 448us/step - loss: 0.0306 - rmse: 0.2490 - val_loss: 0.0449 - val_rmse: 0.3103\n",
      "Epoch 377/1600\n",
      "6573/6573 [==============================] - 3s 477us/step - loss: 0.0303 - rmse: 0.2476 - val_loss: 0.0452 - val_rmse: 0.3123\n",
      "Epoch 378/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0309 - rmse: 0.2504 - val_loss: 0.0456 - val_rmse: 0.3136\n",
      "Epoch 379/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0307 - rmse: 0.2499 - val_loss: 0.0470 - val_rmse: 0.3176\n",
      "Epoch 380/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0275 - rmse: 0.23656573/6573 [==============================] - 3s 474us/step - loss: 0.0307 - rmse: 0.2491 - val_loss: 0.0454 - val_rmse: 0.3128\n",
      "Epoch 381/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0307 - rmse: 0.2497 - val_loss: 0.0452 - val_rmse: 0.3116\n",
      "Epoch 382/1600\n",
      "6573/6573 [==============================] - 3s 449us/step - loss: 0.0297 - rmse: 0.2457 - val_loss: 0.0462 - val_rmse: 0.3159\n",
      "Epoch 383/1600\n",
      "6573/6573 [==============================] - 3s 448us/step - loss: 0.0316 - rmse: 0.2538 - val_loss: 0.0502 - val_rmse: 0.3283\n",
      "Epoch 384/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0307 - rmse: 0.2499 - val_loss: 0.0460 - val_rmse: 0.3146\n",
      "Epoch 385/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0305 - rmse: 0.25106573/6573 [==============================] - 3s 478us/step - loss: 0.0305 - rmse: 0.2489 - val_loss: 0.0477 - val_rmse: 0.3201\n",
      "Epoch 386/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0306 - rmse: 0.2489 - val_loss: 0.0467 - val_rmse: 0.3177\n",
      "Epoch 387/1600\n",
      "6573/6573 [==============================] - 3s 466us/step - loss: 0.0299 - rmse: 0.2465 - val_loss: 0.0450 - val_rmse: 0.3106\n",
      "Epoch 388/1600\n",
      "6573/6573 [==============================] - 3s 478us/step - loss: 0.0304 - rmse: 0.2485 - val_loss: 0.0464 - val_rmse: 0.3159\n",
      "Epoch 389/1600\n",
      "6573/6573 [==============================] - 3s 443us/step - loss: 0.0296 - rmse: 0.2452 - val_loss: 0.0463 - val_rmse: 0.3153\n",
      "Epoch 390/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0350 - rmse: 0.27276573/6573 [==============================] - 3s 460us/step - loss: 0.0303 - rmse: 0.2485 - val_loss: 0.0457 - val_rmse: 0.3131\n",
      "Epoch 391/1600\n",
      "6573/6573 [==============================] - 3s 476us/step - loss: 0.0298 - rmse: 0.2464 - val_loss: 0.0464 - val_rmse: 0.3159\n",
      "Epoch 392/1600\n",
      "6573/6573 [==============================] - 3s 478us/step - loss: 0.0300 - rmse: 0.2466 - val_loss: 0.0453 - val_rmse: 0.3116\n",
      "Epoch 393/1600\n",
      "6573/6573 [==============================] - 3s 466us/step - loss: 0.0299 - rmse: 0.2465 - val_loss: 0.0469 - val_rmse: 0.3177\n",
      "Epoch 394/1600\n",
      "6573/6573 [==============================] - 3s 480us/step - loss: 0.0303 - rmse: 0.2479 - val_loss: 0.0449 - val_rmse: 0.3112\n",
      "Epoch 395/1600\n",
      " 160/6573 [..............................] - ETA: 3s - loss: 0.0344 - rmse: 0.26626573/6573 [==============================] - 3s 468us/step - loss: 0.0304 - rmse: 0.2490 - val_loss: 0.0471 - val_rmse: 0.3180\n",
      "Epoch 396/1600\n",
      "6573/6573 [==============================] - 3s 440us/step - loss: 0.0302 - rmse: 0.2476 - val_loss: 0.0448 - val_rmse: 0.3109\n",
      "Epoch 397/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.0298 - rmse: 0.2462 - val_loss: 0.0447 - val_rmse: 0.3106\n",
      "Epoch 398/1600\n",
      "6573/6573 [==============================] - 3s 476us/step - loss: 0.0304 - rmse: 0.2488 - val_loss: 0.0448 - val_rmse: 0.3109\n",
      "Epoch 399/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0305 - rmse: 0.2490 - val_loss: 0.0457 - val_rmse: 0.3134\n",
      "Epoch 400/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0243 - rmse: 0.22206573/6573 [==============================] - 3s 471us/step - loss: 0.0308 - rmse: 0.2504 - val_loss: 0.0457 - val_rmse: 0.3137\n",
      "Epoch 401/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0299 - rmse: 0.2464 - val_loss: 0.0452 - val_rmse: 0.3120\n",
      "Epoch 402/1600\n",
      "6573/6573 [==============================] - 3s 454us/step - loss: 0.0292 - rmse: 0.2429 - val_loss: 0.0526 - val_rmse: 0.3371\n",
      "Epoch 403/1600\n",
      "6573/6573 [==============================] - 3s 432us/step - loss: 0.0290 - rmse: 0.2431 - val_loss: 0.0474 - val_rmse: 0.3189\n",
      "Epoch 404/1600\n",
      "6573/6573 [==============================] - 3s 471us/step - loss: 0.0296 - rmse: 0.2449 - val_loss: 0.0447 - val_rmse: 0.3100\n",
      "Epoch 405/1600\n",
      " 544/6573 [=>............................] - ETA: 2s - loss: 0.0322 - rmse: 0.25946573/6573 [==============================] - 3s 475us/step - loss: 0.0295 - rmse: 0.2448 - val_loss: 0.0471 - val_rmse: 0.3178\n",
      "Epoch 406/1600\n",
      "6573/6573 [==============================] - 3s 477us/step - loss: 0.0297 - rmse: 0.2460 - val_loss: 0.0461 - val_rmse: 0.3144\n",
      "Epoch 407/1600\n",
      "6573/6573 [==============================] - 3s 462us/step - loss: 0.0298 - rmse: 0.2465 - val_loss: 0.0451 - val_rmse: 0.3112\n",
      "Epoch 408/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0291 - rmse: 0.2433 - val_loss: 0.0477 - val_rmse: 0.3191\n",
      "Epoch 409/1600\n",
      "6573/6573 [==============================] - 3s 452us/step - loss: 0.0294 - rmse: 0.2448 - val_loss: 0.0481 - val_rmse: 0.3212\n",
      "Epoch 410/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0309 - rmse: 0.25186573/6573 [==============================] - 3s 430us/step - loss: 0.0287 - rmse: 0.2408 - val_loss: 0.0442 - val_rmse: 0.3075\n",
      "Epoch 411/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0287 - rmse: 0.2419 - val_loss: 0.0457 - val_rmse: 0.3128\n",
      "Epoch 412/1600\n",
      "6573/6573 [==============================] - 3s 469us/step - loss: 0.0289 - rmse: 0.2426 - val_loss: 0.0464 - val_rmse: 0.3154\n",
      "Epoch 413/1600\n",
      "6573/6573 [==============================] - 3s 489us/step - loss: 0.0297 - rmse: 0.2457 - val_loss: 0.0453 - val_rmse: 0.3118\n",
      "Epoch 414/1600\n",
      "6573/6573 [==============================] - 3s 477us/step - loss: 0.0289 - rmse: 0.2421 - val_loss: 0.0451 - val_rmse: 0.3112\n",
      "Epoch 415/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0246 - rmse: 0.22416573/6573 [==============================] - 3s 483us/step - loss: 0.0299 - rmse: 0.2466 - val_loss: 0.0479 - val_rmse: 0.3207\n",
      "Epoch 416/1600\n",
      "6573/6573 [==============================] - 3s 445us/step - loss: 0.0297 - rmse: 0.2457 - val_loss: 0.0469 - val_rmse: 0.3181\n",
      "Epoch 417/1600\n",
      "6573/6573 [==============================] - 3s 457us/step - loss: 0.0291 - rmse: 0.2434 - val_loss: 0.0448 - val_rmse: 0.3101\n",
      "Epoch 418/1600\n",
      "6573/6573 [==============================] - 3s 484us/step - loss: 0.0293 - rmse: 0.2444 - val_loss: 0.0465 - val_rmse: 0.3162\n",
      "Epoch 419/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0285 - rmse: 0.2404 - val_loss: 0.0449 - val_rmse: 0.3113\n",
      "Epoch 420/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0248 - rmse: 0.22216573/6573 [==============================] - 3s 477us/step - loss: 0.0287 - rmse: 0.2407 - val_loss: 0.0458 - val_rmse: 0.3139\n",
      "Epoch 421/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.0294 - rmse: 0.2440 - val_loss: 0.0447 - val_rmse: 0.3100\n",
      "Epoch 422/1600\n",
      "6573/6573 [==============================] - 3s 467us/step - loss: 0.0293 - rmse: 0.2439 - val_loss: 0.0486 - val_rmse: 0.3241\n",
      "Epoch 423/1600\n",
      "6573/6573 [==============================] - 3s 441us/step - loss: 0.0294 - rmse: 0.2444 - val_loss: 0.0489 - val_rmse: 0.3247\n",
      "Epoch 424/1600\n",
      "6573/6573 [==============================] - 3s 448us/step - loss: 0.0281 - rmse: 0.2386 - val_loss: 0.0507 - val_rmse: 0.3310\n",
      "Epoch 425/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0291 - rmse: 0.24406573/6573 [==============================] - 3s 475us/step - loss: 0.0291 - rmse: 0.2434 - val_loss: 0.0452 - val_rmse: 0.3122\n",
      "Epoch 426/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.0293 - rmse: 0.2436 - val_loss: 0.0473 - val_rmse: 0.3191\n",
      "Epoch 427/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.0296 - rmse: 0.2453 - val_loss: 0.0468 - val_rmse: 0.3166\n",
      "Epoch 428/1600\n",
      "6573/6573 [==============================] - 3s 472us/step - loss: 0.0286 - rmse: 0.2408 - val_loss: 0.0476 - val_rmse: 0.3201\n",
      "Epoch 429/1600\n",
      "6573/6573 [==============================] - 3s 458us/step - loss: 0.0286 - rmse: 0.2414 - val_loss: 0.0450 - val_rmse: 0.3115\n",
      "Epoch 430/1600\n",
      " 416/6573 [>.............................] - ETA: 2s - loss: 0.0297 - rmse: 0.24626573/6573 [==============================] - 3s 433us/step - loss: 0.0296 - rmse: 0.2451 - val_loss: 0.0463 - val_rmse: 0.3161\n",
      "Epoch 431/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0287 - rmse: 0.2419 - val_loss: 0.0441 - val_rmse: 0.3081\n",
      "Epoch 432/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0288 - rmse: 0.2416 - val_loss: 0.0466 - val_rmse: 0.3173\n",
      "Epoch 433/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.0294 - rmse: 0.2442 - val_loss: 0.0450 - val_rmse: 0.3115\n",
      "Epoch 434/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0289 - rmse: 0.2422 - val_loss: 0.0488 - val_rmse: 0.3251\n",
      "Epoch 435/1600\n",
      "  32/6573 [..............................] - ETA: 4s - loss: 0.0484 - rmse: 0.31876573/6573 [==============================] - 3s 479us/step - loss: 0.0292 - rmse: 0.2431 - val_loss: 0.0455 - val_rmse: 0.3131\n",
      "Epoch 436/1600\n",
      "6573/6573 [==============================] - 3s 460us/step - loss: 0.0285 - rmse: 0.2410 - val_loss: 0.0462 - val_rmse: 0.3156\n",
      "Epoch 437/1600\n",
      "6573/6573 [==============================] - 3s 440us/step - loss: 0.0286 - rmse: 0.2403 - val_loss: 0.0462 - val_rmse: 0.3154\n",
      "Epoch 438/1600\n",
      "6573/6573 [==============================] - 3s 470us/step - loss: 0.0280 - rmse: 0.2382 - val_loss: 0.0457 - val_rmse: 0.3131\n",
      "Epoch 439/1600\n",
      "6573/6573 [==============================] - 3s 474us/step - loss: 0.0292 - rmse: 0.2433 - val_loss: 0.0456 - val_rmse: 0.3134\n",
      "Epoch 440/1600\n",
      " 288/6573 [>.............................] - ETA: 2s - loss: 0.0258 - rmse: 0.22916573/6573 [==============================] - 3s 472us/step - loss: 0.0282 - rmse: 0.2390 - val_loss: 0.0454 - val_rmse: 0.3120\n",
      "Epoch 441/1600\n",
      "6573/6573 [==============================] - 3s 468us/step - loss: 0.0284 - rmse: 0.2400 - val_loss: 0.0465 - val_rmse: 0.3168\n",
      "Epoch 442/1600\n",
      "6573/6573 [==============================] - 3s 473us/step - loss: 0.0285 - rmse: 0.2404 - val_loss: 0.0460 - val_rmse: 0.3152\n",
      "Epoch 443/1600\n",
      "6573/6573 [==============================] - 3s 446us/step - loss: 0.0291 - rmse: 0.2432 - val_loss: 0.0442 - val_rmse: 0.3081\n",
      "Epoch 444/1600\n",
      "6573/6573 [==============================] - 3s 448us/step - loss: 0.0287 - rmse: 0.2412 - val_loss: 0.0460 - val_rmse: 0.3143\n",
      "Epoch 445/1600\n",
      " 288/6573 [>.............................] - ETA: 3s - loss: 0.0269 - rmse: 0.23366573/6573 [==============================] - 3s 473us/step - loss: 0.0282 - rmse: 0.2388 - val_loss: 0.0450 - val_rmse: 0.3118\n",
      "Epoch 446/1600\n",
      "6573/6573 [==============================] - 3s 481us/step - loss: 0.0283 - rmse: 0.2396 - val_loss: 0.0466 - val_rmse: 0.3178\n",
      "Epoch 447/1600\n",
      "6573/6573 [==============================] - 3s 475us/step - loss: 0.0277 - rmse: 0.2370 - val_loss: 0.0457 - val_rmse: 0.3140\n",
      "Epoch 448/1600\n",
      "6573/6573 [==============================] - 3s 469us/step - loss: 0.0284 - rmse: 0.2399 - val_loss: 0.0444 - val_rmse: 0.3090\n",
      "Epoch 449/1600\n",
      "6573/6573 [==============================] - 3s 477us/step - loss: 0.0275 - rmse: 0.2359 - val_loss: 0.0476 - val_rmse: 0.3202\n",
      "Epoch 450/1600\n",
      "  32/6573 [..............................] - ETA: 4s - loss: 0.0609 - rmse: 0.39116573/6573 [==============================] - 3s 449us/step - loss: 0.0276 - rmse: 0.2362 - val_loss: 0.0445 - val_rmse: 0.3089\n",
      "Epoch 451/1600\n",
      "4128/6573 [=================>............] - ETA: 1s - loss: 0.0286 - rmse: 0.2412"
     ]
    }
   ],
   "source": [
    "history = ann_model.fit(train_features, train_labels,\n",
    "                    epochs=1600, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 944,
     "status": "ok",
     "timestamp": 1533424068645,
     "user": {
      "displayName": "Emily T",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105098215025944251641"
     },
     "user_tz": 240
    },
    "id": "ReTp5b_EZwfR",
    "outputId": "c3d0b548-4d46-48e5-a3db-3465a140f373"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABA8AAAGqCAYAAABkqW0hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff999bc9910>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18,7))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['rmse'], label=\"Train\")\n",
    "plt.plot(history.history['val_rmse'], label=\"Val\")\n",
    "plt.title('RMSE')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label=\"Train\")\n",
    "plt.plot(history.history['val_loss'], label=\"Val\")\n",
    "plt.title('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "TbWw3SPlZy8u"
   },
   "outputs": [],
   "source": [
    "val_pred = ann_model.predict(val_features).reshape(len(val_features),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aDE3opetZ2Uq"
   },
   "source": [
    "#Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "1Sgja416Z4CM"
   },
   "outputs": [],
   "source": [
    "# RMSE\n",
    "val_rmse = np.sqrt(((val_labels-val_pred) ** 2).mean())\n",
    "\n",
    "# Data\n",
    "compare = pd.DataFrame({'true':val_labels, 'pred':val_pred, 'error':abs(val_pred-val_labels)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 433,
     "status": "ok",
     "timestamp": 1533424078104,
     "user": {
      "displayName": "Emily T",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "105098215025944251641"
     },
     "user_tz": 240
    },
    "id": "bVJAyA0iZ6fb",
    "outputId": "e370de8d-0dbd-42d6-bb66-ac74777f711b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('RMSE:', 0.3151478755409372)\n",
      "('Mean error:', 0.20518127337859493, '\\n')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error</th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.277945</td>\n",
       "      <td>0.722055</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.023997</td>\n",
       "      <td>2.576003</td>\n",
       "      <td>2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.483341</td>\n",
       "      <td>-0.616659</td>\n",
       "      <td>-1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.120437</td>\n",
       "      <td>0.179563</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.035521</td>\n",
       "      <td>0.564479</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.027903</td>\n",
       "      <td>0.772097</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.019483</td>\n",
       "      <td>1.880517</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.076010</td>\n",
       "      <td>-0.523990</td>\n",
       "      <td>-0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.037254</td>\n",
       "      <td>1.862746</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.035984</td>\n",
       "      <td>0.335984</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      error      pred  true\n",
       "0  0.277945  0.722055   1.0\n",
       "1  0.023997  2.576003   2.6\n",
       "2  0.483341 -0.616659  -1.1\n",
       "3  0.120437  0.179563   0.3\n",
       "4  0.035521  0.564479   0.6\n",
       "5  0.027903  0.772097   0.8\n",
       "6  0.019483  1.880517   1.9\n",
       "7  0.076010 -0.523990  -0.6\n",
       "8  0.037254  1.862746   1.9\n",
       "9  0.035984  0.335984   0.3"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print Results\n",
    "print(\"RMSE:\", val_rmse)\n",
    "print('Mean error:', compare['error'].mean(), '\\n')\n",
    "compare.head(10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "(1)emily tried to do shit but broke the notebook.ipynb",
   "provenance": [
    {
     "file_id": "1GPwb4BsdAipZJECAsfdus-ndWi-vMXNc",
     "timestamp": 1533435934880
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
